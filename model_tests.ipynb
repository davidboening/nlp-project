{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "\n",
    "from transformers import EncoderDecoderModel, AutoTokenizer, GenerationConfig, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from tokenizers import processors\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoders\n",
    "    - BERT_JA : `cl-tohoku/bert-base-japanese-v3`\n",
    "    - BERT_EN : `bert-base-uncased`, `prajjwal1/bert-tiny`\n",
    "- Decorders\n",
    "    - GPT_JA : `rinna/japanese-gpt2-xsmall`\n",
    "    - GPT_EN : `gpt2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.7.crossattention.q_attn.bias', 'h.2.crossattention.c_proj.bias', 'h.0.crossattention.q_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_proj.bias', 'h.0.crossattention.c_attn.bias', 'h.5.crossattention.q_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.10.ln_cross_attn.weight', 'h.1.ln_cross_attn.weight', 'h.7.ln_cross_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.3.ln_cross_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.0.ln_cross_attn.bias', 'h.1.ln_cross_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.5.ln_cross_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.5.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.2.crossattention.q_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.8.crossattention.c_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.10.ln_cross_attn.bias', 'h.11.ln_cross_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.0.ln_cross_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.1.crossattention.q_attn.bias', 'h.2.ln_cross_attn.bias', 'h.9.ln_cross_attn.bias', 'h.9.crossattention.q_attn.bias', 'h.6.crossattention.c_proj.bias', 'h.4.crossattention.c_attn.weight', 'h.6.ln_cross_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.6.crossattention.c_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.bias', 'h.10.crossattention.c_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.4.ln_cross_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.11.ln_cross_attn.weight', 'h.10.crossattention.q_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.ln_cross_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.7.ln_cross_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.4.ln_cross_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.bias', 'h.4.crossattention.q_attn.bias', 'h.8.ln_cross_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.4.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.weight', 'h.5.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.weight', 'h.9.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.6.crossattention.q_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "source_lng = \"ja\"\n",
    "\n",
    "if source_lng == \"en\":\n",
    "    target_lng = \"ja\"\n",
    "    encoder = \"bert-base-uncased\"\n",
    "    decoder = \"rinna/japanese-gpt2-small\"\n",
    "else: \n",
    "    target_lng = \"en\"\n",
    "    encoder = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "    decoder = \"gpt2\"\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder, decoder, encoder_add_pooling_layer=False\n",
    ")\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  263,423,232 (1,004.9MB)\n",
      "Cross-attention parameters:   28,366,848 (  108.2MB)\n"
     ]
    }
   ],
   "source": [
    "def print_model_parameters():\n",
    "    t_pars, t_bytes = 0, 0\n",
    "    for p in model.parameters():\n",
    "        t_pars += p.nelement()\n",
    "        t_bytes += p.nelement() * p.element_size()\n",
    "\n",
    "    c_attn_pars, c_attn_bytes = 0, 0\n",
    "    for layer in model.decoder.transformer.h:\n",
    "        for p in layer.crossattention.parameters():\n",
    "            c_attn_pars += p.nelement()\n",
    "            c_attn_bytes += p.nelement() * p.element_size()\n",
    "        for p in layer.ln_cross_attn.parameters():\n",
    "            c_attn_pars += p.nelement()\n",
    "            c_attn_bytes += p.nelement() * p.element_size()\n",
    "\n",
    "    print(f\"Total number of parameters: {t_pars:12,} ({(t_bytes / 1024**2):7,.1f}MB)\")\n",
    "    print(f\"Cross-attention parameters: {c_attn_pars:12,} ({(c_attn_bytes / 1024**2):7,.1f}MB)\")\n",
    "\n",
    "print_model_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_tokenizer = AutoTokenizer.from_pretrained(encoder, use_fast=True)\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(decoder, use_fast=True)\n",
    "if decoder_tokenizer.pad_token_id is None:\n",
    "    decoder_tokenizer.pad_token_id = decoder_tokenizer.eos_token_id\n",
    "\n",
    "model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "model.config.pad_token_id = decoder_tokenizer.eos_token_id\n",
    "\n",
    "# add EOS token at the end of each sentence\n",
    "decoder_tokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A \" + decoder_tokenizer.eos_token,\n",
    "    special_tokens=[(decoder_tokenizer.eos_token, decoder_tokenizer.eos_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing statistic for 100,000 sentences:\n",
      "\ten[tokens] : Avg.  9.10 | Min.     5 | Max.    20 | >32.       0 | >64.     0 | >128.     0 | >256.     0\n",
      "\tja[tokens] : Avg. 12.39 | Min.     4 | Max.    31 | >32.       0 | >64.     0 | >128.     0 | >256.     0\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import SnowSimplifiedDataset\n",
    "dataset = SnowSimplifiedDataset.load()\n",
    "if source_lng == \"ja\":\n",
    "    SnowSimplifiedDataset.stats(en_tokenizer=decoder_tokenizer, ja_tokenizer=encoder_tokenizer)\n",
    "else:\n",
    "    SnowSimplifiedDataset.stats(en_tokenizer=encoder_tokenizer, ja_tokenizer=decoder_tokenizer)\n",
    "\n",
    "dataset = dataset[\"train\"]\n",
    "\n",
    "# sample first chuck of data for testing\n",
    "train = dataset.select(range(1000))\n",
    "valid = dataset.select(range(1000, 1300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGHT = 32\n",
    "\n",
    "def preprocess_data(batch):\n",
    "    inputs = encoder_tokenizer(\n",
    "        batch[f\"{source_lng}_sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGHT,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    labels = decoder_tokenizer(\n",
    "        batch[f\"{target_lng}_sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGHT,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"labels\"] = labels.input_ids\n",
    "    batch[\"labels\"][labels[\"attention_mask\"]==0] = -100\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "train_data = train.map(\n",
    "    preprocess_data, \n",
    "    batched=True, batch_size=64, \n",
    "    remove_columns=[\"en_sentence\", \"ja_sentence\"]\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", \n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "valid_data = valid.map(\n",
    "    preprocess_data, \n",
    "    batched=True, batch_size=64, \n",
    "    remove_columns=[\"en_sentence\", \"ja_sentence\"]\n",
    ")\n",
    "valid_data.set_format(\n",
    "    type=\"torch\", \n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    preds_ids, labels_ids = preds\n",
    "\n",
    "    labels_ids[labels_ids == -100] = decoder_tokenizer.eos_token_id\n",
    "    references = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    references = [[reference] for reference in references]\n",
    "\n",
    "    predictions = decoder_tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
    "\n",
    "    if target_lng == \"ja\":\n",
    "        bleu_output = metric.compute(references=references, predictions=predictions, tokenize=\"ja-mecab\")\n",
    "    else:\n",
    "        bleu_output = metric.compute(references=references, predictions=predictions)\n",
    "    return bleu_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_decoder_configuration(gc: GenerationConfig):\n",
    "    gc.no_repeat_ngram_size = 3\n",
    "    gc.length_penalty = 2.0\n",
    "    gc.num_beams = 3\n",
    "    #gen_config.max_new_tokens = MAX_LENGHT\n",
    "    gc.max_length = MAX_LENGHT * 2\n",
    "    gc.min_length = 0\n",
    "    gc.early_stopping = True\n",
    "    gc.pad_token_id = decoder_tokenizer.eos_token_id\n",
    "    gc.bos_token_id = decoder_tokenizer.bos_token_id\n",
    "    gc.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "    return gc\n",
    "\n",
    "gen_config = GenerationConfig()\n",
    "gen_config = set_decoder_configuration(gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"eval-testing-1\",\n",
    "    num_train_epochs=5,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "\n",
    "    output_dir=\"./.ckp/\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=4,\n",
    "\n",
    "    optim=\"adamw_torch\",\n",
    "    bf16=True,\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_config=gen_config,\n",
    "    # torch_compile=True,\n",
    "    # label_smoothing_factor=0,\n",
    "    # auto_find_batch_size=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args=train_args, \n",
    "    train_dataset=train_data, \n",
    "    eval_dataset=valid_data, \n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavidboening\u001b[0m (\u001b[33mdandd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\src\\nlp-project\\wandb\\run-20230811_165905-boyuyheo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dandd/huggingface/runs/boyuyheo' target=\"_blank\">eval-testing-1</a></strong> to <a href='https://wandb.ai/dandd/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dandd/huggingface' target=\"_blank\">https://wandb.ai/dandd/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dandd/huggingface/runs/boyuyheo' target=\"_blank\">https://wandb.ai/dandd/huggingface/runs/boyuyheo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1002d681a94a6caa53abdc5d01a11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\src\\nlp-project\\venv\\lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8869, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7807b912b54edd8cec639d0eefde51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.9126644134521484, 'eval_score': 0.5865561135849071, 'eval_counts': [461, 14, 4, 0], 'eval_totals': [2285, 1985, 1685, 1385], 'eval_precisions': [20.175054704595187, 0.7052896725440806, 0.23738872403560832, 0.036101083032490974], 'eval_bp': 0.9925877820461596, 'eval_sys_len': 2285, 'eval_ref_len': 2302, 'eval_runtime': 16.351, 'eval_samples_per_second': 18.348, 'eval_steps_per_second': 2.324, 'epoch': 0.96}\n",
      "{'loss': 2.963, 'learning_rate': 3.6666666666666666e-05, 'epoch': 1.28}\n",
      "{'loss': 2.6026, 'learning_rate': 3e-05, 'epoch': 1.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc69fb8d30a54e35aad8a65de815bcf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7908589839935303, 'eval_score': 0.45424080556392626, 'eval_counts': [441, 10, 2, 0], 'eval_totals': [2178, 1878, 1578, 1278], 'eval_precisions': [20.24793388429752, 0.5324813631522897, 0.1267427122940431, 0.03912363067292645], 'eval_bp': 0.9446573913851566, 'eval_sys_len': 2178, 'eval_ref_len': 2302, 'eval_runtime': 12.94, 'eval_samples_per_second': 23.184, 'eval_steps_per_second': 2.937, 'epoch': 1.98}\n",
      "{'loss': 2.3403, 'learning_rate': 2.3333333333333336e-05, 'epoch': 2.56}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c74105eeb4c4cb5b5c00cca850c59a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.757530927658081, 'eval_score': 1.0492544269491462, 'eval_counts': [488, 21, 6, 2], 'eval_totals': [2133, 1833, 1533, 1233], 'eval_precisions': [22.878574777308955, 1.1456628477905073, 0.3913894324853229, 0.16220600162206], 'eval_bp': 0.9238263759026545, 'eval_sys_len': 2133, 'eval_ref_len': 2302, 'eval_runtime': 15.562, 'eval_samples_per_second': 19.278, 'eval_steps_per_second': 2.442, 'epoch': 2.94}\n",
      "{'loss': 2.1607, 'learning_rate': 1.6666666666666667e-05, 'epoch': 3.2}\n",
      "{'loss': 2.0094, 'learning_rate': 1e-05, 'epoch': 3.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0596570e300e4b60939af5461da4c4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.75868821144104, 'eval_score': 1.25598793081679, 'eval_counts': [495, 31, 8, 2], 'eval_totals': [2070, 1770, 1470, 1170], 'eval_precisions': [23.91304347826087, 1.7514124293785311, 0.54421768707483, 0.17094017094017094], 'eval_bp': 0.8939751553008631, 'eval_sys_len': 2070, 'eval_ref_len': 2302, 'eval_runtime': 15.367, 'eval_samples_per_second': 19.522, 'eval_steps_per_second': 2.473, 'epoch': 3.97}\n",
      "{'loss': 1.9146, 'learning_rate': 3.3333333333333333e-06, 'epoch': 4.48}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130caa57adc04cfabd69deff22219e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7598557472229004, 'eval_score': 0.6597507712672607, 'eval_counts': [485, 26, 3, 0], 'eval_totals': [2097, 1797, 1497, 1197], 'eval_precisions': [23.12827849308536, 1.4468558708959376, 0.20040080160320642, 0.04177109440267335], 'eval_bp': 0.9068677018936455, 'eval_sys_len': 2097, 'eval_ref_len': 2302, 'eval_runtime': 16.298, 'eval_samples_per_second': 18.407, 'eval_steps_per_second': 2.332, 'epoch': 4.8}\n",
      "{'train_runtime': 174.7395, 'train_samples_per_second': 28.614, 'train_steps_per_second': 0.429, 'train_loss': 2.50732053120931, 'epoch': 4.8}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=2.50732053120931, metrics={'train_runtime': 174.7395, 'train_samples_per_second': 28.614, 'train_steps_per_second': 0.429, 'train_loss': 2.50732053120931, 'epoch': 4.8})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ids(batch):\n",
    "    pass\n",
    "    batch[\"gen_ids\"] = model.generate(\n",
    "        batch[\"input_ids\"].cuda(),\n",
    "        attention_mask=batch[\"attention_mask\"].cuda()\n",
    "    )\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "model.eval()\n",
    "train_out = train_data.map(\n",
    "    generate_ids, \n",
    "    batched=True, batch_size=16,\n",
    "    remove_columns=[\"input_ids\", \"attention_mask\"]\n",
    ")\n",
    "valid_out = valid_data.map(\n",
    "    generate_ids, \n",
    "    batched=True, batch_size=16,\n",
    "    remove_columns=[\"input_ids\", \"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 1.1970624690859315, 'counts': [1577, 88, 22, 9], 'totals': [7609, 6609, 5609, 4609], 'precisions': [20.725456696017872, 1.3315176274776819, 0.3922267783918702, 0.19527012367107832], 'bp': 0.9927977789048245, 'sys_len': 7609, 'ref_len': 7664}\n"
     ]
    }
   ],
   "source": [
    "print(compute_metrics((train_out[\"gen_ids\"], train_out[\"labels\"])))\n",
    "\n",
    "train_output = decoder_tokenizer.batch_decode(train_out[\"gen_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column gen_ids not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels', 'gen_sentence']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m compute_metrics((valid_out[\u001b[39m\"\u001b[39;49m\u001b[39mgen_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m], valid_out[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[0;32m      3\u001b[0m valid_output \u001b[39m=\u001b[39m decoder_tokenizer\u001b[39m.\u001b[39mbatch_decode(valid_out[\u001b[39m\"\u001b[39m\u001b[39mgen_ids\u001b[39m\u001b[39m\"\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\src\\nlp-project\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2803\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2801\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2802\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[1;32mc:\\src\\nlp-project\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2787\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2785\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m   2786\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m-> 2787\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m   2788\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[0;32m   2789\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[0;32m   2790\u001b[0m )\n\u001b[0;32m   2791\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\src\\nlp-project\\venv\\lib\\site-packages\\datasets\\formatting\\formatting.py:580\u001b[0m, in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    578\u001b[0m     _raise_bad_key_type(key)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 580\u001b[0m     _check_valid_column_key(key, table\u001b[39m.\u001b[39;49mcolumn_names)\n\u001b[0;32m    581\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    582\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n",
      "File \u001b[1;32mc:\\src\\nlp-project\\venv\\lib\\site-packages\\datasets\\formatting\\formatting.py:520\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[1;34m(key, columns)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_valid_column_key\u001b[39m(key: \u001b[39mstr\u001b[39m, columns: List[\u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m columns:\n\u001b[1;32m--> 520\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[39m{\u001b[39;00mcolumns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Column gen_ids not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels', 'gen_sentence']\""
     ]
    }
   ],
   "source": [
    "compute_metrics((valid_out[\"gen_ids\"], valid_out[\"labels\"]))\n",
    "\n",
    "valid_output = decoder_tokenizer.batch_decode(valid_out[\"gen_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pairs(dataset, generation, sample=10):\n",
    "    assert len(dataset) == len(generation), \"Invalid combination!\"\n",
    "    target = dataset[f\"{target_lng}_sentence\"]\n",
    "    source = dataset[f\"{source_lng}_sentence\"]\n",
    "\n",
    "    sample_ids = random.sample(len(dataset), sample)\n",
    "    for i, tpl in enumerate(zip(source[sample_ids], target[sample_ids], generation[sample_ids])):\n",
    "        print(f\"Sentence #{i} [id={sample_ids[i]}]\\n\")\n",
    "        print(f\"Original:  {tpl[0]}\\n\\tTarget:    {tpl[1]}\\n\\tGenerated: {tpl[2]}\\n\")\n",
    "    return\n",
    "\n",
    "print_pairs(valid[f\"{target_lng}_sentence\"], valid_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
