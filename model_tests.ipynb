{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "from transformers import EncoderDecoderModel, AutoTokenizer, GenerationConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoders\n",
    "    - BERT_JA : `cl-tohoku/bert-base-japanese-v3`\n",
    "    - BERT_EN : `bert-base-uncased`, `prajjwal1/bert-tiny`\n",
    "- Decorders\n",
    "    - GPT_JA : `rinna/japanese-gpt2-xsmall`\n",
    "    - GPT_EN : `gpt2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lng = \"ja\"\n",
    "target_lng = \"en\"\n",
    "\n",
    "if source_lng == \"en\":\n",
    "    encoder = \"bert-base-uncased\"\n",
    "    decoder = \"rinna/japanese-gpt2-xsmall\"\n",
    "else: \n",
    "    encoder = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "    decoder = \"gpt2\"\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder, decoder, encoder_add_pooling_layer=False\n",
    ")\n",
    "model.cuda()\n",
    "\n",
    "encoder_tokenizer = AutoTokenizer.from_pretrained(encoder, use_fast=True)\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(decoder, use_fast=True)\n",
    "decoder_tokenizer.pad_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters():\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    c_attn_pars = 0\n",
    "    for layer in model.decoder.transformer.h:\n",
    "        c_attn_pars += sum(p.numel() for p in layer.crossattention.parameters())\n",
    "        c_attn_pars += sum(p.numel() for p in layer.ln_cross_attn.parameters())\n",
    "\n",
    "    print(f\"Number of cross-attention parameters: {c_attn_pars}\")\n",
    "\n",
    "\n",
    "print_model_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size():\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print(\"model size: {:.1f}MB\".format(size_all_mb))\n",
    "\n",
    "\n",
    "print_model_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=r\"./data-csv/snow_simplified.csv\")\n",
    "data_sample = dataset[\"train\"]\n",
    "data_sample = data_sample.select(range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to test with jp tokenizer\n",
    "# need to test with samples containing multiple sentences\n",
    "\n",
    "from tokenizers import processors\n",
    "decoder_tokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A \" + decoder_tokenizer.eos_token,\n",
    "    special_tokens=[(decoder_tokenizer.eos_token, decoder_tokenizer.eos_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(batch):\n",
    "    inputs = encoder_tokenizer(\n",
    "    batch[f\"{source_lng}_sentence\"],\n",
    "    padding=\"max_length\",\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    labels = decoder_tokenizer(\n",
    "        batch[f\"{target_lng}_sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"labels\"] = labels\n",
    "    batch[\"labels\"][batch[\"labels\"]==decoder_tokenizer.pad_token_id] = -100\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_sample.map(preprocess_data, batched=True, remove_columns=[\"en_sentence\", \"ja_sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"labels\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    output_dir=\"./\",\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(model, args=train_args, train_dataset=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "model.config.pad_token_id = decoder_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = GenerationConfig()\n",
    "\n",
    "def set_decoder_configuration(gen_config):\n",
    "    gen_config.no_repeat_ngram_size = 3\n",
    "    gen_config.length_penalty = 2.0\n",
    "    gen_config.num_beams = 4\n",
    "    gen_config.max_new_tokens = 128\n",
    "    gen_config.early_stopping = True\n",
    "    gen_config.pad_token_id = decoder_tokenizer.eos_token_id\n",
    "    gen_config.bos_token_id = decoder_tokenizer.bos_token_id\n",
    "    gen_config.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "    return gen_config\n",
    "\n",
    "\n",
    "gen_config = set_decoder_configuration(gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "model.eval()\n",
    "output = model.generate(\n",
    "    train_data[\"input_ids\"].cuda(),\n",
    "    attention_mask=train_data[\"attention_mask\"].cuda(),\n",
    "    generation_config=gen_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(output.size()[0]):\n",
    "    print(output[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_output = decoder_tokenizer.batch_decode(output, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample[\"en_sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
