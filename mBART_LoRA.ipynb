{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, \\\n",
    "    GenerationConfig, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "# from transformers.utils import logging\n",
    "# logging.set_verbosity_info()\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "\n",
    "SOURCE_LANG = \"en\"\n",
    "\n",
    "if SOURCE_LANG == \"en\":\n",
    "    TARGET_LANG = \"ja\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ja_XX\")\n",
    "else: \n",
    "    TARGET_LANG = \"en\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"ja_XX\", tgt_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_count, trainable_bytes = 0, 0\n",
    "    total_count, total_bytes = 0, 0\n",
    "    for _, param in model.named_parameters():\n",
    "        total_count += param.nelement()\n",
    "        total_bytes += param.nelement() * param.element_size()\n",
    "        if param.requires_grad:\n",
    "            trainable_count += param.nelement()\n",
    "            trainable_bytes += param.nelement() * param.element_size()\n",
    "    print(\n",
    "        f\"Total params: {total_count:12,} ({(total_bytes / 1024**2):7,.1f}MB) | \"\n",
    "        f\"Trainable params: {trainable_count:12,} ({(trainable_bytes / 1024**2):7,.1f}MB) [{100 * trainable_count / total_count:3.1f}%]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_to_save = [\"final_layer_norm\", \"self_attn_layer_norm\", \"layer_norm\", \"layernorm_embedding\", \"embed_positions\"]\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\", \"shared\", \"lm_head\"]\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8, # anything is fine (simply tune lr)\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules,\n",
    "    modules_to_save=modules_to_save, # may modify the base model\n",
    "    bias=\"all\", # may modify the base model (\"lora_only\" or \"all\")\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "lora_model = get_peft_model(model, config)\n",
    "print_trainable_parameters(lora_model)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import EnJaDatasetMaker, EnJaDatasetSample, WikiCorpus\n",
    "dataset = EnJaDatasetMaker.prepare_dataset(\n",
    "    \"wiki-corpus-test-1\",\n",
    "    [\n",
    "        # lower is inclusive, upper is exclusive (0, 32) -> [0, 31]\n",
    "        EnJaDatasetSample(WikiCorpus, 1000, (64, 128)),\n",
    "    ],\n",
    "    source_language=SOURCE_LANG,\n",
    "    model_type=\"mBART\",\n",
    "    tokenizer=tokenizer,\n",
    "    num_proc=8,\n",
    "    seed=42\n",
    ")\n",
    "train_data = dataset.select(range(700))\n",
    "valid_data = dataset.select(range(700, 1000))\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "if TARGET_LANG == \"ja\":\n",
    "    def compute_metrics(preds):\n",
    "        preds_ids, labels_ids = preds\n",
    "\n",
    "        labels_ids[labels_ids == -100] = tokenizer.eos_token_id\n",
    "        references = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "        references = [[reference] for reference in references]\n",
    "\n",
    "        predictions = tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
    "\n",
    "        bleu_output = metric.compute(\n",
    "            references=references, \n",
    "            predictions=predictions, \n",
    "            tokenize=\"ja-mecab\"\n",
    "        )\n",
    "        return bleu_output\n",
    "else:\n",
    "    def compute_metrics(preds):\n",
    "        preds_ids, labels_ids = preds\n",
    "\n",
    "        labels_ids[labels_ids == -100] = tokenizer.eos_token_id\n",
    "        references = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "        references = [[reference] for reference in references]\n",
    "\n",
    "        predictions = tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
    "        \n",
    "        bleu_output = metric.compute(\n",
    "            references=references, \n",
    "            predictions=predictions\n",
    "        )\n",
    "        return bleu_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGHT = 256\n",
    "def set_decoder_configuration(gc: GenerationConfig):\n",
    "    gc.no_repeat_ngram_size = 4\n",
    "    gc.length_penalty = 2.0\n",
    "    gc.num_beams = 3\n",
    "    #gen_config.max_new_tokens = MAX_LENGHT\n",
    "    gc.max_length = MAX_LENGHT\n",
    "    gc.min_length = 0\n",
    "    gc.early_stopping = True\n",
    "    # pad token is set to eos since in GPT2 pad does not exist\n",
    "    gc.pad_token_id = tokenizer.eos_token_id\n",
    "    gc.bos_token_id = tokenizer.bos_token_id\n",
    "    gc.eos_token_id = tokenizer.eos_token_id\n",
    "    return gc\n",
    "\n",
    "gen_config = GenerationConfig()\n",
    "gen_config = set_decoder_configuration(gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"testing-lora-1\",\n",
    "    num_train_epochs=4,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    \n",
    "    # remove_unused_columns=False,\n",
    "\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    \n",
    "    predict_with_generate=True,\n",
    "    # include_inputs_for_metrics=\"True\"\n",
    "    generation_config=gen_config,\n",
    "\n",
    "    output_dir=\"./.ckp/\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=4,\n",
    "\n",
    "    optim=\"adamw_torch\",\n",
    "    bf16=True,\n",
    "    \n",
    "    group_by_length=True,\n",
    "    length_column_name=\"length\",\n",
    "\n",
    "    # torch_compile=True,\n",
    "    label_smoothing_factor=0.1,\n",
    "    \n",
    "    auto_find_batch_size=True,\n",
    "    # per_device_train_batch_size=8,\n",
    "    # per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    # eval_accumulation_steps=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    lora_model, \n",
    "    args=train_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.cuda()\n",
    "lora_model.eval()\n",
    "\n",
    "train_out = trainer.predict(train_data)\n",
    "valid_out = trainer.predict(valid_data)\n",
    "\n",
    "print(\"Train:\", train_out.metrics)\n",
    "print(\"Valid:\", valid_out.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decode = tokenizer.batch_decode(train_out.predictions, skip_special_tokens=True)\n",
    "valid_decode = tokenizer.batch_decode(valid_out.predictions, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pairs(dataset, generation, sample=5):\n",
    "    assert len(dataset) == len(generation), \"Invalid combination!\"\n",
    "\n",
    "    sample_ids = random.sample(range(len(dataset)), sample)\n",
    "    for i, sid in enumerate(sample_ids):\n",
    "        print(f\"Sentence #{i} [id={sid}]\")\n",
    "        print(\n",
    "            f\"\\tOriginal:  {dataset['source'][sid]}\\n\"\n",
    "            f\"\\tTarget:    {dataset['target'][sid]}\\n\"\n",
    "            f\"\\tGenerated: {generation[sid]}\\n\"\n",
    "        )\n",
    "    return\n",
    "\n",
    "print_pairs(valid_data, valid_decode, sample=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
