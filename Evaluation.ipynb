{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "import pathlib\n",
    "import re\n",
    "import json\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, \\\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, \\\n",
    "    EncoderDecoderModel, AutoTokenizer\n",
    "from tokenizers import processors\n",
    "from peft import PeftModel\n",
    "from utils.metric import SacreBleu\n",
    "from utils.dataset import Flores, WMTvat, EnJaDatasetMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_dataset(name, src_lang, type, encoder_tokenizer=None, decoder_tokenizer=None):\n",
    "    trg_lang = \"ja\" if src_lang == \"en\" else \"en\"\n",
    "    \n",
    "    if name == \"flores_dev\":\n",
    "        data = Flores.load(\"dev\").rename_columns({f\"{src_lang}_sentence\": \"source\", f\"{trg_lang}_sentence\": \"target\"})\n",
    "    elif name == \"wmt_vat\":\n",
    "        data = WMTvat.load(f\"{src_lang}-{trg_lang}\").rename_columns({f\"{src_lang}_sentence\": \"source\", f\"{trg_lang}_sentence\": \"target\"})\n",
    "    else: raise ValueError()\n",
    "\n",
    "    if type == \"mBART\":\n",
    "        data = data.map(\n",
    "            EnJaDatasetMaker._get_map_compute_mBART_tokenization(\n",
    "                tokenizer=encoder_tokenizer\n",
    "            )\n",
    "        )\n",
    "    elif type.startswith(\"BERT-GPT2\"):\n",
    "        data = data.map(\n",
    "            EnJaDatasetMaker._get_map_compute_BERT_GPT2_tokenization(\n",
    "                encoder_tokenizer=encoder_tokenizer, decoder_tokenizer=decoder_tokenizer\n",
    "            )\n",
    "        )\n",
    "    else: raise ValueError()\n",
    "    return data\n",
    "\n",
    "def get_tokenizer(type, src_lang):\n",
    "    trg_lang = \"ja\" if src_lang == \"en\" else \"en\"\n",
    "    \n",
    "    if type == \"mBART\":\n",
    "        tok = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=f\"{src_lang}_XX\", tgt_lang=f\"{trg_lang}_XX\")\n",
    "        return {\n",
    "            \"encoder_tokenizer\": tok,\n",
    "            \"decoder_tokenizer\": tok            \n",
    "        }\n",
    "    elif type.startswith(\"BERT-GPT2\"):\n",
    "        if src_lang == \"en\":\n",
    "            encoder = \"bert-base-uncased\"\n",
    "            decoder = \"rinna/japanese-gpt2-small\"\n",
    "        else: # src_lang == \"ja\"\n",
    "            encoder = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "            decoder = \"gpt2\"\n",
    "        tok = {\n",
    "            \"encoder_tokenizer\": AutoTokenizer.from_pretrained(encoder, use_fast=True),\n",
    "            \"decoder_tokenizer\": AutoTokenizer.from_pretrained(decoder, use_fast=True)\n",
    "        }\n",
    "        if tok[\"decoder_tokenizer\"].pad_token_id is None:\n",
    "            tok[\"decoder_tokenizer\"].pad_token_id = tok[\"decoder_tokenizer\"].eos_token_id\n",
    "        tok[\"decoder_tokenizer\"]._tokenizer.post_processor = processors.TemplateProcessing(\n",
    "            single=\"$A \" + tok[\"decoder_tokenizer\"].eos_token,\n",
    "            special_tokens=[(tok[\"decoder_tokenizer\"].eos_token, tok[\"decoder_tokenizer\"].eos_token_id)],\n",
    "        )\n",
    "        return tok\n",
    "    else: raise ValueError()\n",
    "\n",
    "def get_base_model(type, src_lang):\n",
    "    trg_lang = \"ja\" if src_lang == \"en\" else \"en\"\n",
    "    \n",
    "    if type == \"mBART\":\n",
    "        model =  MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "    elif type == \"BERT-GPT2-LoRA\":\n",
    "        if src_lang == \"en\":\n",
    "            encoder = \"bert-base-uncased\"\n",
    "            decoder = \"rinna/japanese-gpt2-small\"\n",
    "        else: # src_lang == \"ja\"\n",
    "            encoder = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "            decoder = \"gpt2\"\n",
    "        model = EncoderDecoderModel.from_encoder_decoder_pretrained(encoder, decoder)\n",
    "    elif type == \"BERT-GPT2-xattn-LoRA\":\n",
    "        path_to_ckp = f\"./.ckp/{src_lang}-{trg_lang}-BERT-GPT2-xattn/checkpoint-{25000}\"\n",
    "        model = EncoderDecoderModel.from_pretrained(path_to_ckp, local_files_only=True)\n",
    "    elif type == \"BERT-GPT2\":\n",
    "        model = None\n",
    "    else: raise ValueError()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ckp_scores(model, path_to_ckp, dataset, tokenizer, trg_lang):\n",
    "    if model is not None:\n",
    "        model = PeftModel.from_pretrained(model=model, model_id=path_to_ckp)\n",
    "    else:\n",
    "        model = EncoderDecoderModel.from_pretrained(path_to_ckp, local_files_only=True)\n",
    "    \n",
    "    metrics = SacreBleu.get_mBART_metric(tokenizer=tokenizer, target_language=trg_lang)\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    train_args = Seq2SeqTrainingArguments(\n",
    "        report_to=\"none\",\n",
    "        prediction_loss_only=False,\n",
    "        predict_with_generate=True,\n",
    "        bf16=True,\n",
    "        group_by_length=True,\n",
    "        output_dir=\"./.ckp\",\n",
    "        length_column_name=\"length\",\n",
    "        label_smoothing_factor=0.2,\n",
    "        per_device_eval_batch_size=8\n",
    "    )\n",
    "    gen_config = {\n",
    "        \"max_length\" : 256,\n",
    "        \"early_stopping\" : True,\n",
    "        \"no_repeat_ngram_size\" : 4,\n",
    "        \"length_penalty\" : 1.0,\n",
    "        \"num_beams\" : 5\n",
    "    }\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args=train_args,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=metrics\n",
    "    )\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    score = trainer.predict(dataset, **gen_config).metrics\n",
    "        \n",
    "    return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_config_scores(type, ckp_name, src_lang, dataset_name, last_only=False):\n",
    "    assert type in [\"mBART\", \"BERT-GPT2\", \"BERT-GPT2-LoRA\", \"BERT-GPT2-xattn-LoRA\"], \"invalid type\"\n",
    "    assert os.path.exists(f\"./.ckp/{ckp_name}\"), \"invalid ckp id\"\n",
    "    assert src_lang in [\"en\", \"ja\"], \"invalid language\"\n",
    "    assert dataset_name in [\"flores_dev\", \"wmt_vat\"], \"invalid dataset\"\n",
    "    trg_lang = \"ja\" if src_lang == \"en\" else \"en\"\n",
    "    \n",
    "    # get all checkpoints\n",
    "    ckps = []\n",
    "    p_num = re.compile(\".*-(.*)$\")\n",
    "    for fname in pathlib.Path(f\"./.ckp/{ckp_name}\").glob(\"*\"):\n",
    "        ckps.append(int(p_num.match(str(fname)).groups()[0]))\n",
    "    ckps.sort()\n",
    "    if last_only: ckps = ckps[-1]\n",
    "    \n",
    "    # generate tokenizer, dataset, model\n",
    "    tokenizers = get_tokenizer(type, src_lang)\n",
    "    model = get_base_model(type, src_lang=src_lang)\n",
    "    if type == \"BERT-GPT2-LoRA\":\n",
    "        model.config.decoder_start_token_id = tokenizers[\"decoder_tokenizer\"].bos_token_id\n",
    "        model.config.eos_token_id = tokenizers[\"decoder_tokenizer\"].eos_token_id\n",
    "        model.config.pad_token_id = tokenizers[\"decoder_tokenizer\"].pad_token_id\n",
    "    \n",
    "    dataset = get_eval_dataset(name=dataset_name, src_lang=src_lang, type=type, **tokenizers)\n",
    "    \n",
    "    path_to_save = f\"./eval/{ckp_name}/{dataset_name}.json\"\n",
    "    if not os.path.exists(f\"./eval/{ckp_name}\"):\n",
    "        os.makedirs(f\"./eval/{ckp_name}\")\n",
    "    scores = {}\n",
    "    if os.path.isfile(path_to_save): # if exists resume\n",
    "        with open(path_to_save, \"r\") as fp:\n",
    "            scores = json.load(fp)\n",
    "    \n",
    "    for ckp in ckps:\n",
    "        if str(ckp) in scores:\n",
    "            continue\n",
    "        \n",
    "        path_to_ckp = f\"./.ckp/{ckp_name}/checkpoint-{ckp}\"\n",
    "        path_to_save = f\"./eval/{ckp_name}/{dataset_name}.json\"\n",
    "        metrics = compute_ckp_scores(model, path_to_ckp, dataset, tokenizers[\"decoder_tokenizer\"], trg_lang=trg_lang)\n",
    "        scores[ckp] = metrics\n",
    "\n",
    "        with open(path_to_save, \"w\") as fp:\n",
    "            fp.write(json.dumps(scores))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = compute_config_scores(\"BERT-GPT2\", \"en-ja-BERT-GPT2-xattn\", \"en\", \"flores_dev\", last_only=False)\n",
    "# scores = compute_config_scores(\"BERT-GPT2-xattn-LoRA\", \"en-ja-BERT-GPT2-xattn-LoRA\", \"en\", \"flores_dev\", last_only=False)\n",
    "scores = compute_config_scores(\"mBART\", \"en-ja-mixed-250k+bt-250k\", \"en\", \"flores_dev\", last_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores2 = compute_config_scores(\"BERT-GPT2-LoRA\", \"en-ja-BERT-GPT2-LoRA\", \"en\", \"flores_dev\", last_only=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def generate_res():\n",
    "    ckp_name = \"en-ja-ckp-25000-bt-500k\"\n",
    "    src_lang, trg_lang = \"en\", \"ja\"\n",
    "    ckp = 40000\n",
    "    type = \"mBART\"\n",
    "    dataset = \"wmt_vat\"\n",
    "    \n",
    "    ###\n",
    "    # ckp_name = \"en-ja-BERT-GPT2-xattn\"\n",
    "    # src_lang, trg_lang = \"en\", \"ja\"\n",
    "    # ckp = 10000\n",
    "    # type = \"BERT-GPT2\"\n",
    "    # dataset = \"wmt_vat\"\n",
    "    \n",
    "    tokenizer = get_tokenizer(type, src_lang)\n",
    "    dataset = get_eval_dataset(dataset, src_lang, type, **tokenizer)\n",
    "    model = get_base_model(type, src_lang)\n",
    "    \n",
    "    path_to_ckp = f\"./.ckp/{ckp_name}/checkpoint-{ckp}\"\n",
    "    \n",
    "    if model is not None:\n",
    "        model = PeftModel.from_pretrained(model=model, model_id=path_to_ckp)\n",
    "    else:\n",
    "        model = EncoderDecoderModel.from_pretrained(path_to_ckp, local_files_only=True)\n",
    "\n",
    "    metrics = SacreBleu.get_mBART_metric(tokenizer=tokenizer[\"decoder_tokenizer\"], target_language=trg_lang)\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer[\"decoder_tokenizer\"], model=model)\n",
    "\n",
    "    train_args = Seq2SeqTrainingArguments(\n",
    "        report_to=\"none\",\n",
    "        prediction_loss_only=False,\n",
    "        predict_with_generate=True,\n",
    "        bf16=True,\n",
    "        group_by_length=True,\n",
    "        output_dir=\"./.ckp\",\n",
    "        length_column_name=\"length\",\n",
    "        label_smoothing_factor=0.2,\n",
    "        per_device_eval_batch_size=8\n",
    "    )\n",
    "    gen_config = {\n",
    "        \"max_length\" : 256,\n",
    "        \"early_stopping\" : True,\n",
    "        \"no_repeat_ngram_size\" : 4,\n",
    "        \"length_penalty\" : 1.0,\n",
    "        \"num_beams\" : 5\n",
    "    }\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args=train_args,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=metrics\n",
    "    )\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    predictions = trainer.predict(dataset, **gen_config).predictions\n",
    "    \n",
    "    predictions_decode = tokenizer[\"decoder_tokenizer\"].batch_decode(predictions, skip_special_tokens=True)\n",
    "    return dataset, predictions_decode\n",
    "\n",
    "data, predictions_decode = generate_res()\n",
    "\n",
    "\n",
    "\n",
    "from textwrap import wrap\n",
    "def print_pairs(dataset, generation, sample=5):\n",
    "    assert len(dataset) == len(generation), \"Invalid combination!\"\n",
    "\n",
    "    sample_ids = random.sample(range(len(dataset)), sample)\n",
    "    for i, sid in enumerate(sample_ids):\n",
    "        print(f\"Sentence #{i} [id={sid}]\")\n",
    "        print(\n",
    "            \"\\n\\t\\t\\t\".join(wrap(f\"\\tOriginal:  {dataset['source'][sid]}\", width=100)),\n",
    "            \"\\n\\t\\t\\t\".join(wrap(f\"\\tTarget:    {dataset['target'][sid]}\", width=100)),\n",
    "            \"\\n\\t\\t\\t\".join(wrap(f\"\\tGenerated: {generation[sid]}\", width=100)), sep=\"\\n\"\n",
    "        )\n",
    "        print(\"\\n\")\n",
    "    return\n",
    "\n",
    "print_pairs(data, predictions_decode, sample=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
