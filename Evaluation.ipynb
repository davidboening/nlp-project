{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, \\\n",
    "    GenerationConfig, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DATASET_NAME = \"mixed-500k\"\n",
    "# TRAIN_DATASET_NAME = \"news-250k\"\n",
    "TRAIN_DATASET_NAME = \"mixed-250k+bt-250k\"\n",
    "\n",
    "SOURCE_LANG = \"en\"\n",
    "# SOURCE_LANG = \"ja\"\n",
    "\n",
    "if SOURCE_LANG == \"en\":\n",
    "    TARGET_LANG = \"ja\"\n",
    "else: \n",
    "    TARGET_LANG = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "model_path = f\"{SOURCE_LANG}-{TARGET_LANG}-{TRAIN_DATASET_NAME}\"\n",
    "ckp_path = f\"./.ckp/{model_path}\"\n",
    "eval_path = f\"./.eval/{model_path}\"\n",
    "\n",
    "pathlib.Path(eval_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=f\"{SOURCE_LANG}_XX\", tgt_lang=f\"{TARGET_LANG}_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import EnJaDatasetMaker\n",
    "dataset = EnJaDatasetMaker.load_dataset(model_path)\n",
    "train_data = dataset[\"train\"]\n",
    "valid_data = dataset[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metric import SacreBleu\n",
    "compute_metrics = SacreBleu.get_mBART_metric(tokenizer=tokenizer, target_language=TARGET_LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    report_to=\"none\",\n",
    "\n",
    "    prediction_loss_only=False,\n",
    "    predict_with_generate=True,\n",
    "\n",
    "    bf16=True, # bf16, qint 8 ???\n",
    "    output_dir=\"./ckp\",\n",
    "    \n",
    "    group_by_length=True,\n",
    "    length_column_name=\"length\",\n",
    "\n",
    "    label_smoothing_factor=0.2, # 0.1, 0.2\n",
    "    \n",
    "    per_device_eval_batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of datasets\n",
    "datasets = {}\n",
    "\n",
    "# add validation set if wanted\n",
    "# datasets[\"valid\"] = valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import Flores\n",
    "\n",
    "flores_dev_data = Flores.load(\"dev\").rename_columns({f\"{SOURCE_LANG}_sentence\": \"source\", f\"{TARGET_LANG}_sentence\": \"target\"})\n",
    "\n",
    "flores_dev_data = flores_dev_data.map(\n",
    "    EnJaDatasetMaker._get_map_compute_mBART_tokenization(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "datasets[\"flores_dev\"] = flores_dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flores_test_data = Flores.load(\"dev\").rename_columns({f\"{SOURCE_LANG}_sentence\": \"source\", f\"{TARGET_LANG}_sentence\": \"target\"})\n",
    "\n",
    "flores_test_data = flores_test_data.map(\n",
    "    EnJaDatasetMaker._get_map_compute_mBART_tokenization(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "# datasets[\"flores_test\"] = flores_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import WMTvat\n",
    "\n",
    "wmt_data = WMTvat.load(f\"{SOURCE_LANG}-{TARGET_LANG}\").rename_columns({f\"{SOURCE_LANG}_sentence\": \"source\", f\"{TARGET_LANG}_sentence\": \"target\"})\n",
    "\n",
    "wmt_data = wmt_data.map(\n",
    "    EnJaDatasetMaker._get_map_compute_mBART_tokenization(tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "# datasets[\"wmt\"] = wmt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint evaluation\n",
    "Evaluate sacreBLEU score on all checkpoints saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import json\n",
    "\n",
    "# dictionary of evaluation results\n",
    "eval_sets = {}\n",
    "\n",
    "keys = datasets.keys()\n",
    "# keys = [\"flores_dev\"]\n",
    "\n",
    "# initialize with keys in datasets dictionary\n",
    "for eval_key in keys:\n",
    "    \n",
    "    # load previously existing results if present\n",
    "    if os.path.isfile(f\"{eval_path}/{eval_key}_results.json\"):\n",
    "        with open(f\"{eval_path}/{eval_key}_results.json\") as f:\n",
    "            eval_sets[eval_key] = json.load(f)\n",
    "    else:\n",
    "        eval_sets[eval_key] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define checkpoints to evaluate based on model type and source language\n",
    "\n",
    "if TRAIN_DATASET_NAME == \"mixed-500k\":\n",
    "    checkpoints = range(5000, 55000, 5000)\n",
    "elif TRAIN_DATASET_NAME == \"news-250k\":\n",
    "    if SOURCE_LANG == \"en\":\n",
    "        checkpoints = range(3500, 24500, 3500)\n",
    "    else:\n",
    "        checkpoints = list(range(3750, 22500, 3750))\n",
    "        checkpoints.append(20000)\n",
    "        checkpoints.append(21250)\n",
    "elif TRAIN_DATASET_NAME == \"mixed-250k+bt-250k\":\n",
    "    checkpoints = range(2500, 27500, 2500)\n",
    "else:\n",
    "    raise ValueError()\n",
    "\n",
    "\n",
    "# list of checkpoints to manually define. Comment/uncomment based on needs\n",
    "checkpoints = [25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = {\n",
    "    \"max_length\" : 256,\n",
    "    \"early_stopping\" : True,\n",
    "    \n",
    "    \"no_repeat_ngram_size\" : 4,\n",
    "    \"length_penalty\" : 1.0,\n",
    "    \n",
    "    \"num_beams\" : 5,\n",
    "    # \"num_beam_groups\" : 5,\n",
    "    # \"diversity_penalty\" : 0.5,\n",
    "    # \"do_sample\" : True,\n",
    "    # \"penalty_alpha\" : 0.6,\n",
    "    # \"top_k\" : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle to keep generated predictions during evaluation\n",
    "# these will not be saved to disk currently\n",
    "# TODO maybe possible to save them\n",
    "KEEP_PREDICTIONS = True\n",
    "\n",
    "predictions_dict = {}\n",
    "\n",
    "if KEEP_PREDICTIONS:\n",
    "    for pred_key in keys:\n",
    "        predictions_dict[pred_key] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in checkpoints:\n",
    "\n",
    "    lora_model = PeftModel.from_pretrained(model=model, model_id=f\"{ckp_path}/checkpoint-{i}/\")\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=lora_model)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        lora_model,\n",
    "        args=train_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=valid_data,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    lora_model.cuda()\n",
    "    lora_model.eval()\n",
    "\n",
    "    # evaluate on each dataset and save to file\n",
    "    # inefficient to save at each checkpoint as it overwrites the file\n",
    "    # but can be stopped without losing progress\n",
    "\n",
    "    # WARNING: results are not necessarily ordered by key if partial results were loaded\n",
    "    for eval_key in eval_sets.keys():\n",
    "\n",
    "        print(f\"Evaluating {eval_key} dataset...\")\n",
    "        \n",
    "        predictions = trainer.predict(datasets[eval_key], **gen_config)\n",
    "        \n",
    "        if KEEP_PREDICTIONS:\n",
    "            predictions_dict[eval_key][f\"{i}\"] = predictions.predictions\n",
    "\n",
    "        eval_sets[eval_key][f\"{i}\"] = predictions.metrics\n",
    "\n",
    "        with open(f\"{eval_path}/{eval_key}_results.json\", \"w\") as f:\n",
    "            f.write(json.dumps(eval_sets[eval_key]))\n",
    "\n",
    "    print(\"Checkpoint \", i, \" DONE\")\n",
    "    del lora_model, trainer, data_collator, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample examination\n",
    "Translate a given dataset with the chosen checkpoint to examine get sacreBLEU score and examine translation quality on samples of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 25000\n",
    "dataset = \"flores_dev\"\n",
    "data = datasets[dataset]\n",
    "\n",
    "if KEEP_PREDICTIONS == False:\n",
    "    predictions = predictions_dict[dataset][f\"{checkpoint}\"]\n",
    "    metrics = eval_sets[dataset][f\"{checkpoint}\"]\n",
    "\n",
    "else:\n",
    "    lora_model = PeftModel.from_pretrained(model=model, model_id=f\"{ckp_path}/checkpoint-{checkpoint}/\")\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=lora_model)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        lora_model,\n",
    "        args=train_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=valid_data,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    lora_model.cuda()\n",
    "    lora_model.eval()\n",
    "\n",
    "    outputs = trainer.predict(data, **gen_config)\n",
    "    predictions = outputs.predictions\n",
    "    metrics = outputs.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics: \", metrics)\n",
    "predictions_decode = tokenizer.batch_decode(predictions, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "def print_pairs(dataset, generation, sample=5):\n",
    "    assert len(dataset) == len(generation), \"Invalid combination!\"\n",
    "\n",
    "    sample_ids = random.sample(range(len(dataset)), sample)\n",
    "    for i, sid in enumerate(sample_ids):\n",
    "        print(f\"Sentence #{i} [id={sid}]\")\n",
    "        print(\n",
    "            \"\\n\\t\\t\\t\".join(wrap(f\"\\tOriginal:  {dataset['source'][sid]}\", width=100)),\n",
    "            \"\\n\\t\\t\\t\".join(wrap(f\"\\tTarget:    {dataset['target'][sid]}\", width=100)),\n",
    "            \"\\n\\t\\t\\t\".join(wrap(f\"\\tGenerated: {generation[sid]}\", width=100)), sep=\"\\n\"\n",
    "        )\n",
    "        print(\"\\n\")\n",
    "    return\n",
    "\n",
    "print_pairs(data, predictions_decode, sample=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
