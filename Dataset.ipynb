{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloads and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage: https://nlp.stanford.edu/projects/jesc/\n",
      "Paper  : https://arxiv.org/abs/1710.10639\n",
      "Summary: Japanese-English Subtitle Corpus (2.8M sentences)\n",
      "skipped: jesc.csv file already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'en_sentence': \"you are back, aren't you, harold?\",\n",
       " 'ja_sentence': 'あなたは戻ったのね ハロルド?'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.dataset import JESC\n",
    "\n",
    "JESC.info()\n",
    "JESC.create_csv()\n",
    "JESC.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage : https://github.com/venali/BilingualCorpus/\n",
      "Summary : a large scale corpus of manually translated Japanese sentences\n",
      "          extracted from Wikipedia's Kyoto Articles (~500k sentences)\n",
      "skipped: wiki_corpus.csv file already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'en_sentence': 'Sesshu', 'ja_sentence': '雪舟'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.dataset import WikiCorpus\n",
    "\n",
    "WikiCorpus.info()\n",
    "WikiCorpus.create_csv()\n",
    "WikiCorpus.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage    : https://opus.nlpl.eu/Tatoeba.php\n",
      "Webpage(HF): https://huggingface.co/datasets/tatoeba\n",
      "Summary    : a collection of sentences from https://tatoeba.org/en/, contains\n",
      "             over 400 languages ([en-ja] 200k sentences)\n",
      "skipped: tatoeba.csv file already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'en_sentence': \"Let's try something.\", 'ja_sentence': '何かしてみましょう。'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.dataset import Tatoeba\n",
    "\n",
    "Tatoeba.info()\n",
    "Tatoeba.create_csv()\n",
    "Tatoeba.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage: https://huggingface.co/datasets/snow_simplified_japanese_corpus\n",
      "Summary: Japanese-English sentence pairs, all Japanese sentences have\n",
      "         a simplified counterpart (85k(x2) sentences)\n",
      "skipped: snow_simplified.csv file already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'en_sentence': \"i can 't tell who will arrive first .\",\n",
       " 'ja_sentence': '誰が一番に着くか私には分かりません。'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.dataset import SnowSimplified\n",
    "\n",
    "SnowSimplified.info()\n",
    "SnowSimplified.create_csv()\n",
    "SnowSimplified.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage: https://huggingface.co/datasets/Amani27/massive_translation_dataset\n",
      "Summary: dataset derived from AmazonScience/MASSIVE for translation\n",
      "         (16k sentences in 10 languages)\n",
      "skipped: massive_translation.csv file already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'en_sentence': 'wake me up at nine am on friday',\n",
       " 'ja_sentence': '金曜日の午前九時に起こしてください'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.dataset import MassiveTranslation\n",
    "\n",
    "MassiveTranslation.info()\n",
    "MassiveTranslation.create_csv()\n",
    "MassiveTranslation.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage    : https://sites.google.com/site/iwsltevaluation2017/TED-tasks\n",
      "Webpage(HF): https://huggingface.co/datasets/iwslt2017\n",
      "Summary    : a collection of multilingual tasks, one of which is a bilingual\n",
      "             corpus of 230k [en-ja] sentences.\n",
      "skipped: iwslt2017.csv file already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'en_sentence': \"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\",\n",
       " 'ja_sentence': 'どうもありがとう クリス このステージに立てる機会を 2度もいただけるというのは実に光栄なことで とてもうれしく思っています'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.dataset import IWSLT2017\n",
    "\n",
    "IWSLT2017.info()\n",
    "IWSLT2017.create_csv()\n",
    "IWSLT2017.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage    : https://github.com/EdinburghNLP/opus-100-corpus\n",
      "Webpage(HF): https://huggingface.co/datasets/opus100\n",
      "Summary    : a multilingual corpus with 1M [en-ja] sentences,\n",
      "             of various origins.\n",
      "skipped: opus100.csv file already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'en_sentence': 'Yeah, Vincent Hanna.',\n",
       " 'ja_sentence': '- ラウール - ラウールに ヴィンセント・ハンナだ'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.dataset import OPUS100\n",
    "\n",
    "OPUS100.info()\n",
    "OPUS100.create_csv()\n",
    "OPUS100.load()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage: https://github.com/facebookresearch/flores/tree/main/flores200\n",
      "Paper  : https://arxiv.org/abs/2207.04672\n",
      "Summary: Professional translation in over 200 languages, including\n",
      "         en-ja, for evaluation tasks.\n",
      "skipped: ('flores.dev.csv', 'flores.devtest.csv') file already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ja_sentence': '月曜日にスタンフォード大学医学部の科学者たちは、細胞を種類別に分類できる新しい診断ツールを発明したと発表しました。それは標準的なインクジェットプリンタで印刷して製造できる小型チップであり、原価は1枚あたり1円ほどす。',\n",
       " 'en_sentence': 'On Monday, scientists from the Stanford University School of Medicine announced the invention of a new diagnostic tool that can sort cells by type: a tiny printable chip that can be manufactured using standard inkjet printers for possibly about one U.S. cent each.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.dataset import Flores\n",
    "\n",
    "Flores.info()\n",
    "Flores.create_csv()\n",
    "Flores.load(\"dev\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webpage    : https://huggingface.co/datasets/gsarti/wmt_vat\n",
      "Paper      : https://openreview.net/forum?id=hhKA5k0oVy5Summary    : A filtered version of WMT dataset increasing correlation with human\n",
      "             judgement. Contains ja-en, en-ja professional translations for evaluation tasks\n",
      "skipped: ('wmt_vat.en.ja.csv', 'wmt_vat.ja.en.csv') file already exists!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ja_sentence': '\"嫌いな女ランキング\"2位の鈴木奈々 、 \" 失礼すぎるボディタッチ\"などしくじりエピソードを告白（ザテレビジョン ） - Yahoo!ニュース',\n",
       " 'en_sentence': 'Suzuki Nana, 2nd place in the ranking of disliked women, talks about her various gaffes, such as extremely rude body touching (The Television) - Yahoo!News'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.dataset import WMTvat\n",
    "\n",
    "WMTvat.info()\n",
    "WMTvat.create_csv()\n",
    "WMTvat.load(\"en-ja\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Dataset Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "\n",
    "from utils.dataset.dataset_base import EnJaDataset\n",
    "\n",
    "def get_csv_path(cls):\n",
    "    assert issubclass(cls, EnJaDataset), \"Invalid class passed!\"\n",
    "    return f\"{EnJaDataset.DATASET_PROCESSED_DIR}/{cls.OUT_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import processors\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "source_lng = \"ja\"\n",
    "\n",
    "if source_lng == \"en\": \n",
    "    target_lng = \"ja\"\n",
    "    encoder = \"bert-base-uncased\"\n",
    "    decoder = \"rinna/japanese-gpt2-small\" \n",
    "else: \n",
    "    target_lng = \"en\"\n",
    "    encoder = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "    decoder = \"gpt2\"\n",
    "\n",
    "encoder_tokenizer = AutoTokenizer.from_pretrained(encoder, use_fast=True)\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(decoder, use_fast=True)\n",
    "if decoder_tokenizer.pad_token_id is None:\n",
    "    decoder_tokenizer.pad_token_id = decoder_tokenizer.eos_token_id\n",
    "\n",
    "# adds an EOS token at the end of each sentence\n",
    "decoder_tokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A \" + decoder_tokenizer.eos_token,\n",
    "    special_tokens=[(decoder_tokenizer.eos_token, decoder_tokenizer.eos_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: loaded dataset with id=\"ja-en-BERT-GPT2-test\" from existing cache.\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import EnJaDatasetMaker, EnJaDatasetSample, SnowSimplified, MassiveTranslation\n",
    "\n",
    "dataset = EnJaDatasetMaker.prepare_dataset(\n",
    "    \"ja-en-BERT-GPT2-test\",\n",
    "    [\n",
    "        # lower is inclusive, upper is exclusive (0, 32) -> [0, 31]\n",
    "        EnJaDatasetSample(get_csv_path(SnowSimplified),      124, (0, 64)),\n",
    "        EnJaDatasetSample(get_csv_path(MassiveTranslation),   50, (0, 32)),\n",
    "    ],\n",
    "    source_language=source_lng,\n",
    "    model_type=\"BERT-GPT2\",\n",
    "    encoder_tokenizer=encoder_tokenizer,\n",
    "    decoder_tokenizer=decoder_tokenizer,\n",
    "    num_proc=6,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['target', 'source', 'length', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 174\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = EnJaDatasetMaker.load_dataset(\"ja-en-BERT-GPT2-test\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 'upon my word i will do it .',\n",
       " 'source': '誓って私はそれをします。',\n",
       " 'length': tensor(11),\n",
       " 'input_ids': tensor([    2, 29062,   456,  4262,   465, 12546,   500,   441, 12995,   385,\n",
       "             3]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([27287,   616,  1573,  1312,   481,   466,   340,   764, 50256])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBart50TokenizerFast\n",
    "\n",
    "source_lng = \"en\"\n",
    "\n",
    "if source_lng == \"en\":\n",
    "    target_lng = \"ja\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ja_XX\")\n",
    "else: \n",
    "    target_lng = \"en\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"ja_XX\", tgt_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: loaded dataset with id=\"en-ja-mBART-test\" from existing cache.\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import EnJaDatasetMaker, EnJaDatasetSample, SnowSimplified, MassiveTranslation\n",
    "\n",
    "dataset = EnJaDatasetMaker.prepare_dataset(\n",
    "    \"en-ja-mBART-test\",\n",
    "    [\n",
    "        # lower is inclusive, upper is exclusive (0, 32) -> [0, 31]\n",
    "        EnJaDatasetSample(get_csv_path(SnowSimplified),      124, (0, 64)),\n",
    "        EnJaDatasetSample(get_csv_path(MassiveTranslation),   50, (0, 32)),\n",
    "    ],\n",
    "    source_language=source_lng,\n",
    "    model_type=\"mBART\",\n",
    "    tokenizer=tokenizer,\n",
    "    num_proc=6,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'target', 'length', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 174\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = EnJaDatasetMaker.load_dataset(\"en-ja-mBART-test\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'upon my word i will do it .',\n",
       " 'target': '誓って私はそれをします。',\n",
       " 'length': tensor(11),\n",
       " 'input_ids': tensor([250004,  54799,    759,   2565,     17,   1221,     54,    442,      6,\n",
       "              5,      2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([250012,      6, 111891,   2995,  25711,  37741,   5182,     30,      2])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "\n",
    "from utils.dataset import EnJaDatasetMaker, EnJaDatasetSample, JESC, SnowSimplified, MassiveTranslation, Tatoeba, IWSLT2017, OPUS100\n",
    "\n",
    "from transformers import MBart50TokenizerFast\n",
    "def get_mBART_tokenizer(source_language):\n",
    "    if source_language == \"en\":\n",
    "        return MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ja_XX\")\n",
    "    else: # source_language == \"ja\":\n",
    "        return MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"ja_XX\", tgt_lang=\"en_XX\")\n",
    "    \n",
    "def get_BERT_GPT2_tokenizers(source_language):\n",
    "    if source_language == \"en\":\n",
    "        encoder, decoder = \"bert-base-uncased\", \"rinna/japanese-gpt2-small\"\n",
    "    else:\n",
    "        encoder, decoder = \"cl-tohoku/bert-base-japanese-v3\", \"gpt2\"\n",
    "    encoder_tokenizer = AutoTokenizer.from_pretrained(encoder, use_fast=True)\n",
    "    decoder_tokenizer = AutoTokenizer.from_pretrained(decoder, use_fast=True)\n",
    "    if decoder_tokenizer.pad_token_id is None:\n",
    "        decoder_tokenizer.pad_token_id = decoder_tokenizer.eos_token_id\n",
    "    # add EOS token at the end of each sentence\n",
    "    decoder_tokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"$A \" + decoder_tokenizer.eos_token,\n",
    "        special_tokens=[(decoder_tokenizer.eos_token, decoder_tokenizer.eos_token_id)],\n",
    "    )\n",
    "    return encoder_tokenizer, decoder_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. BERT-GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_LANG, TARGET_LANG = \"en\", \"ja\"\n",
    "encoder_tokenizer, decoder_tokenizer = get_BERT_GPT2_tokenizers(SOURCE_LANG)\n",
    "\n",
    "dataset = EnJaDatasetMaker.prepare_dataset(\n",
    "    f\"BERT-GPT2-{SOURCE_LANG}-{TARGET_LANG}\",\n",
    "    [\n",
    "        EnJaDatasetSample(dataset=get_csv_path(OPUS100), nsample=50_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(JESC), nsample=150_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(MassiveTranslation), nsample=20_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(SnowSimplified), nsample=30_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(Tatoeba), nsample=125_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(IWSLT2017), nsample=175_000, ntokens=(0, 128)),\n",
    "    ],\n",
    "    source_language = SOURCE_LANG,\n",
    "    model_type= \"BERT-GPT2\",\n",
    "    encoder_tokenizer = encoder_tokenizer,\n",
    "    decoder_tokenizer= decoder_tokenizer,\n",
    "    num_proc  = 8,\n",
    "    seed      = 123,\n",
    "    splits    = (1, 0.002, 1) # rescaled to 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({k:len(v) for k,v in dataset.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Mixed-250k + 250k Backtranslation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: loaded dataset with id=\"en-ja-mixed-250k+bt-250k\" from existing cache.\n"
     ]
    }
   ],
   "source": [
    "SOURCE_LANGUAGE, TARGET_LANGUAGE = \"en\", \"ja\"\n",
    "tokenizer = get_mBART_tokenizer(SOURCE_LANGUAGE)\n",
    "\n",
    "dataset = EnJaDatasetMaker.prepare_dataset(\n",
    "    f\"{SOURCE_LANGUAGE}-{TARGET_LANGUAGE}-mixed-250k+bt-250k\",\n",
    "    [\n",
    "        EnJaDatasetSample(dataset=get_csv_path(OPUS100), nsample=50_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(JESC), nsample=150_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(MassiveTranslation), nsample=20_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(SnowSimplified), nsample=30_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(Tatoeba), nsample=125_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(IWSLT2017), nsample=175_000, ntokens=(0, 128)),\n",
    "    ],\n",
    "    source_language = SOURCE_LANGUAGE,\n",
    "    model_type= \"mBART\",\n",
    "    tokenizer = tokenizer,\n",
    "    num_proc  = 8,\n",
    "    seed      = 123,\n",
    "    splits    = (1, 0.002, 1) # rescaled to 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: loaded dataset with id=\"ja-en-mixed-250k+bt-250k\" from existing cache.\n"
     ]
    }
   ],
   "source": [
    "SOURCE_LANGUAGE, TARGET_LANGUAGE = \"ja\", \"en\"\n",
    "tokenizer = get_mBART_tokenizer(SOURCE_LANGUAGE)\n",
    "\n",
    "dataset = EnJaDatasetMaker.prepare_dataset(\n",
    "    f\"{SOURCE_LANGUAGE}-{TARGET_LANGUAGE}-mixed-250k+bt-250k\",\n",
    "    [\n",
    "        EnJaDatasetSample(dataset=get_csv_path(OPUS100), nsample=50_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(JESC), nsample=150_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(MassiveTranslation), nsample=20_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(SnowSimplified), nsample=30_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(Tatoeba), nsample=125_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(IWSLT2017), nsample=175_000, ntokens=(0, 128)),\n",
    "    ],\n",
    "    source_language = SOURCE_LANGUAGE,\n",
    "    model_type= \"mBART\",\n",
    "    tokenizer = tokenizer,\n",
    "    num_proc  = 8,\n",
    "    seed      = 123,\n",
    "    splits    = (1, 0.002, 1) # rescaled to 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 272986, 'valid': 541, 'test': 272994}\n"
     ]
    }
   ],
   "source": [
    "print({k:len(v) for k,v in dataset.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Mixed-500k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: loaded dataset with id=\"en-ja-mixed-500k\" from existing cache.\n"
     ]
    }
   ],
   "source": [
    "# valid is too large and test set is not used however we cannot change it\n",
    "SOURCE_LANGUAGE, TARGET_LANGUAGE = \"en\", \"ja\"\n",
    "tokenizer = get_mBART_tokenizer(SOURCE_LANGUAGE)\n",
    "\n",
    "dataset = EnJaDatasetMaker.prepare_dataset(\n",
    "    f\"{SOURCE_LANGUAGE}-{TARGET_LANGUAGE}-mixed-500k\",\n",
    "    [\n",
    "        EnJaDatasetSample(dataset=get_csv_path(OPUS100), nsample=50_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(JESC), nsample=150_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(MassiveTranslation), nsample=20_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(SnowSimplified), nsample=30_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(Tatoeba), nsample=125_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(IWSLT2017), nsample=175_000, ntokens=(0, 128)),\n",
    "    ],\n",
    "    source_language = SOURCE_LANGUAGE,\n",
    "    model_type= \"mBART\",\n",
    "    tokenizer = tokenizer,\n",
    "    num_proc  = 8,\n",
    "    seed      = 42,\n",
    "    splits    = (1, 0.002, 0.01) # rescaled to 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: loaded dataset with id=\"ja-en-mixed-500k\" from existing cache.\n"
     ]
    }
   ],
   "source": [
    "SOURCE_LANGUAGE, TARGET_LANGUAGE = \"ja\", \"en\"\n",
    "tokenizer = get_mBART_tokenizer(SOURCE_LANGUAGE)\n",
    "\n",
    "dataset = EnJaDatasetMaker.prepare_dataset(\n",
    "    f\"{SOURCE_LANGUAGE}-{TARGET_LANGUAGE}-mixed-500k\",\n",
    "    [\n",
    "        EnJaDatasetSample(dataset=get_csv_path(OPUS100), nsample=50_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(JESC), nsample=150_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(MassiveTranslation), nsample=20_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(SnowSimplified), nsample=30_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(Tatoeba), nsample=125_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(IWSLT2017), nsample=175_000, ntokens=(0, 128)),\n",
    "    ],\n",
    "    source_language = SOURCE_LANGUAGE,\n",
    "    model_type= \"mBART\",\n",
    "    tokenizer = tokenizer,\n",
    "    num_proc  = 8,\n",
    "    seed      = 42,\n",
    "    splits    = (1, 0.002, 0.01) # rescaled to 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 540038, 'valid': 1077, 'test': 5406}\n"
     ]
    }
   ],
   "source": [
    "print({k:len(v) for k,v in dataset.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. News-250k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: loaded dataset with id=\"en-ja-news-250k\" from existing cache.\n"
     ]
    }
   ],
   "source": [
    "SOURCE_LANGUAGE, TARGET_LANGUAGE = \"en\", \"ja\"\n",
    "tokenizer = get_mBART_tokenizer(SOURCE_LANGUAGE)\n",
    "\n",
    "dataset = EnJaDatasetMaker.prepare_dataset(\n",
    "    f\"{SOURCE_LANGUAGE}-{TARGET_LANGUAGE}-news-250k\",\n",
    "    [\n",
    "        EnJaDatasetSample(dataset=get_csv_path(IWSLT2017), nsample=250_000, ntokens=(0, 128)),\n",
    "        # EnJaDatasetSample(dataset=Tatoeba, nsample=250_000, ntokens=(0, 128)),\n",
    "    ],\n",
    "    source_language = SOURCE_LANGUAGE,\n",
    "    model_type= \"mBART\",\n",
    "    tokenizer = tokenizer,\n",
    "    num_proc  = 8,\n",
    "    seed      = 42,\n",
    "    splits    = (1, 0.002, 0.01) # rescaled to 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped: loaded dataset with id=\"ja-en-news-250k\" from existing cache.\n"
     ]
    }
   ],
   "source": [
    "SOURCE_LANGUAGE, TARGET_LANGUAGE = \"ja\", \"en\"\n",
    "tokenizer = get_mBART_tokenizer(SOURCE_LANGUAGE)\n",
    "\n",
    "dataset = EnJaDatasetMaker.prepare_dataset(\n",
    "    f\"{SOURCE_LANGUAGE}-{TARGET_LANGUAGE}-news-250k\",\n",
    "    [\n",
    "        EnJaDatasetSample(dataset=get_csv_path(IWSLT2017), nsample=250_000, ntokens=(0, 128)),\n",
    "        # EnJaDatasetSample(dataset=Tatoeba, nsample=250_000, ntokens=(0, 128)),\n",
    "    ],\n",
    "    source_language = SOURCE_LANGUAGE,\n",
    "    model_type= \"mBART\",\n",
    "    tokenizer = tokenizer,\n",
    "    num_proc  = 8,\n",
    "    seed      = 42,\n",
    "    splits    = (1, 0.002, 0.01) # rescaled to 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 229581, 'valid': 459, 'test': 2296}\n"
     ]
    }
   ],
   "source": [
    "print({k:len(v) for k,v in dataset.items()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
