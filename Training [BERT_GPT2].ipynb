{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "\n",
    "from transformers import EncoderDecoderModel, AutoTokenizer, GenerationConfig, Seq2SeqTrainer, \\\n",
    "    Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from tokenizers import processors\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoders\n",
    "    - BERT_JA : `cl-tohoku/bert-base-japanese-v3`\n",
    "    - BERT_EN : `bert-base-uncased`, `prajjwal1/bert-tiny`\n",
    "- Decorders\n",
    "    - GPT_JA : `rinna/japanese-gpt2-xsmall`\n",
    "    - GPT_EN : `gpt2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_LANG = \"en\"\n",
    "RESUME = False\n",
    "\n",
    "if SOURCE_LANG == \"en\":\n",
    "    TARGET_LANG = \"ja\"\n",
    "    encoder = \"bert-base-uncased\"\n",
    "    decoder = \"rinna/japanese-gpt2-small\"\n",
    "else: \n",
    "    TARGET_LANG = \"en\"\n",
    "    encoder = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "    decoder = \"gpt2\"\n",
    "\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder, decoder\n",
    ")\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_parameters():\n",
    "    t_pars, t_bytes = 0, 0\n",
    "    for p in model.parameters():\n",
    "        t_pars += p.nelement()\n",
    "        t_bytes += p.nelement() * p.element_size()\n",
    "\n",
    "    c_attn_pars, c_attn_bytes = 0, 0\n",
    "    for layer in model.decoder.transformer.h:\n",
    "        for p in layer.crossattention.parameters():\n",
    "            c_attn_pars += p.nelement()\n",
    "            c_attn_bytes += p.nelement() * p.element_size()\n",
    "        for p in layer.ln_cross_attn.parameters():\n",
    "            c_attn_pars += p.nelement()\n",
    "            c_attn_bytes += p.nelement() * p.element_size()\n",
    "\n",
    "    print(f\"Total number of parameters: {t_pars:12,} ({(t_bytes / 1024**2):7,.1f}MB)\")\n",
    "    print(f\"Cross-attention parameters: {c_attn_pars:12,} ({(c_attn_bytes / 1024**2):7,.1f}MB)\")\n",
    "\n",
    "print_model_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cross_attention_only(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for layer in model.decoder.transformer.h:\n",
    "        for p in layer.crossattention.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in layer.ln_cross_attn.parameters():\n",
    "            p.requires_grad = True\n",
    "set_cross_attention_only(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_tokenizer = AutoTokenizer.from_pretrained(encoder, use_fast=True)\n",
    "decoder_tokenizer = AutoTokenizer.from_pretrained(decoder, use_fast=True)\n",
    "if decoder_tokenizer.pad_token_id is None:\n",
    "    decoder_tokenizer.pad_token_id = decoder_tokenizer.eos_token_id\n",
    "\n",
    "model.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "model.config.pad_token_id = decoder_tokenizer.eos_token_id\n",
    "\n",
    "# add EOS token at the end of each sentence\n",
    "decoder_tokenizer._tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"$A \" + decoder_tokenizer.eos_token,\n",
    "    special_tokens=[(decoder_tokenizer.eos_token, decoder_tokenizer.eos_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import EnJaDatasetMaker, EnJaDatasetSample, \\\n",
    "    OPUS100, JESC, MassiveTranslation, SnowSimplified, Tatoeba, IWSLT2017\n",
    "from utils.dataset.dataset_base import EnJaDataset\n",
    "\n",
    "def get_csv_path(cls):\n",
    "    assert issubclass(cls, EnJaDataset), \"Invalid class passed!\"\n",
    "    return f\"{EnJaDataset.DATASET_PROCESSED_DIR}/{cls.OUT_NAME}\"\n",
    "\n",
    "dataset = EnJaDatasetMaker.prepare_dataset(\n",
    "    f\"BERT-GPT2-{SOURCE_LANG}-{TARGET_LANG}\",\n",
    "    [\n",
    "        EnJaDatasetSample(dataset=get_csv_path(OPUS100), nsample=50_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(JESC), nsample=150_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(MassiveTranslation), nsample=20_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(SnowSimplified), nsample=30_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(Tatoeba), nsample=125_000, ntokens=(0, 128)),\n",
    "        EnJaDatasetSample(dataset=get_csv_path(IWSLT2017), nsample=175_000, ntokens=(0, 128)),\n",
    "    ],\n",
    "    source_language = SOURCE_LANG,\n",
    "    model_type= \"BERT-GPT2\",\n",
    "    encoder_tokenizer = encoder_tokenizer,\n",
    "    decoder_tokenizer= decoder_tokenizer,\n",
    "    num_proc  = 8,\n",
    "    seed      = 123,\n",
    "    splits    = (1, 0.002, 1) # rescaled to 1\n",
    ")\n",
    "\n",
    "train_data = dataset[\"train\"].remove_columns([\"source\", \"target\"])\n",
    "valid_data = dataset[\"valid\"].remove_columns([\"source\", \"target\"])\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(encoder_tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "if TARGET_LANG == \"ja\":\n",
    "    def compute_metrics(preds):\n",
    "        preds_ids, labels_ids = preds\n",
    "\n",
    "        labels_ids[labels_ids == -100] = decoder_tokenizer.eos_token_id\n",
    "        references = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "        references = [[reference] for reference in references]\n",
    "\n",
    "        predictions = decoder_tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
    "        \n",
    "        bleu_output = metric.compute(\n",
    "            references=references, \n",
    "            predictions=predictions, \n",
    "            tokenize=\"ja-mecab\"\n",
    "        )\n",
    "        return bleu_output\n",
    "else:\n",
    "    def compute_metrics(preds):\n",
    "        preds_ids, labels_ids = preds\n",
    "\n",
    "        labels_ids[labels_ids == -100] = decoder_tokenizer.eos_token_id\n",
    "        references = decoder_tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "        references = [[reference] for reference in references]\n",
    "\n",
    "        predictions = decoder_tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
    "        \n",
    "        bleu_output = metric.compute(\n",
    "            references=references, \n",
    "            predictions=predictions\n",
    "        )\n",
    "        return bleu_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_decoder_configuration(gc: GenerationConfig):\n",
    "    gc.no_repeat_ngram_size = 4\n",
    "    gc.length_penalty = 2.0\n",
    "    gc.num_beams = 3\n",
    "    #gen_config.max_new_tokens = MAX_LENGHT\n",
    "    gc.max_length = 256\n",
    "    gc.min_length = 0\n",
    "    gc.early_stopping = True\n",
    "    # pad token is set to eos since in GPT2 pad does not exist\n",
    "    gc.pad_token_id = decoder_tokenizer.eos_token_id\n",
    "    gc.bos_token_id = decoder_tokenizer.bos_token_id\n",
    "    gc.eos_token_id = decoder_tokenizer.eos_token_id\n",
    "    return gc\n",
    "\n",
    "gen_config = GenerationConfig()\n",
    "gen_config = set_decoder_configuration(gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    run_name=f\"BERT-GPT2-{SOURCE_LANG}-{TARGET_LANG}-xattn\",\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1, # * 4, 2, 1\n",
    "\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2500, # * 20_000, 10_000, 5_000\n",
    "    prediction_loss_only=False,\n",
    "    predict_with_generate=True,\n",
    "    generation_config=gen_config,\n",
    "\n",
    "    output_dir=f\"./.ckp/BERT-GPT2-{SOURCE_LANG}-{TARGET_LANG}-xattn/\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2500, # * 20_000, 10_000, 5_000\n",
    "    save_total_limit=100,\n",
    "    load_best_model_at_end=True, # defaults to metric: \"loss\"\n",
    "    metric_for_best_model=\"eval_score\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_steps=400, # 3500, 1750, 875\n",
    "    learning_rate=5e-5, # 3e-5, 5e-5\n",
    "    bf16=True, # bf16, qint 8 ???\n",
    "    \n",
    "    group_by_length=True,\n",
    "    length_column_name=\"length\",\n",
    "\n",
    "    # torch_compile=True,\n",
    "    label_smoothing_factor=0.2, # 0.1, 0.2\n",
    "    \n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    # gradient_accumulation_steps=1, # * 1, 2, 4\n",
    "    # gradient_checkpointing=True,\n",
    "    # eval_accumulation_steps=4, # ???\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model, \n",
    "    args=train_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data, \n",
    "    eval_dataset=valid_data, \n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "trainer.train(resume_from_checkpoint=RESUME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
