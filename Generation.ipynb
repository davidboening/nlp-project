{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "import pathlib\n",
    "import re\n",
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, \\\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, \\\n",
    "    EncoderDecoderModel, AutoTokenizer\n",
    "from tokenizers import processors\n",
    "from peft import PeftModel\n",
    "# from utils.metric import SacreBleu\n",
    "from utils.dataset import Flores, WMTvat, EnJaDatasetMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(data, src_lang, type, encoder_tokenizer=None, decoder_tokenizer=None):\n",
    "    trg_lang = \"ja\" if src_lang == \"en\" else \"en\"\n",
    "\n",
    "    if type == \"mBART\":\n",
    "        data = data.map(\n",
    "            EnJaDatasetMaker._get_map_compute_mBART_tokenization(\n",
    "                tokenizer=encoder_tokenizer\n",
    "            )\n",
    "        )\n",
    "    elif type.startswith(\"BERT-GPT2\"):\n",
    "        data = data.map(\n",
    "            EnJaDatasetMaker._get_map_compute_BERT_GPT2_tokenization(\n",
    "                encoder_tokenizer=encoder_tokenizer, decoder_tokenizer=decoder_tokenizer\n",
    "            )\n",
    "        )\n",
    "    else: raise ValueError()\n",
    "    return data\n",
    "\n",
    "def get_tokenizer(type, src_lang):\n",
    "    trg_lang = \"ja\" if src_lang == \"en\" else \"en\"\n",
    "    \n",
    "    if type == \"mBART\":\n",
    "        tok = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=f\"{src_lang}_XX\", tgt_lang=f\"{trg_lang}_XX\")\n",
    "        return {\n",
    "            \"encoder_tokenizer\": tok,\n",
    "            \"decoder_tokenizer\": tok            \n",
    "        }\n",
    "    elif type.startswith(\"BERT-GPT2\"):\n",
    "        if src_lang == \"en\":\n",
    "            encoder = \"bert-base-uncased\"\n",
    "            decoder = \"rinna/japanese-gpt2-small\"\n",
    "        else: # src_lang == \"ja\"\n",
    "            encoder = \"cl-tohoku/bert-base-japanese-v3\"\n",
    "            decoder = \"gpt2\"\n",
    "        tok = {\n",
    "            \"encoder_tokenizer\": AutoTokenizer.from_pretrained(encoder, use_fast=True),\n",
    "            \"decoder_tokenizer\": AutoTokenizer.from_pretrained(decoder, use_fast=True)\n",
    "        }\n",
    "        if tok[\"decoder_tokenizer\"].pad_token_id is None:\n",
    "            tok[\"decoder_tokenizer\"].pad_token_id = tok[\"decoder_tokenizer\"].eos_token_id\n",
    "        tok[\"decoder_tokenizer\"]._tokenizer.post_processor = processors.TemplateProcessing(\n",
    "            single=\"$A \" + tok[\"decoder_tokenizer\"].eos_token,\n",
    "            special_tokens=[(tok[\"decoder_tokenizer\"].eos_token, tok[\"decoder_tokenizer\"].eos_token_id)],\n",
    "        )\n",
    "        return tok\n",
    "    else: raise ValueError()\n",
    "\n",
    "def get_base_model(type, src_lang):\n",
    "    trg_lang = \"ja\" if src_lang == \"en\" else \"en\"\n",
    "    \n",
    "    if type == \"mBART\":\n",
    "        model =  MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "    elif type == \"BERT-GPT2-xattn-LoRA\":\n",
    "        path_to_ckp = f\"./.ckp/{src_lang}-{trg_lang}-BERT-GPT2-xattn/checkpoint-{25000}\"\n",
    "        model = EncoderDecoderModel.from_pretrained(path_to_ckp, local_files_only=True)\n",
    "    elif type == \"BERT-GPT2-xattn\":\n",
    "        model = None\n",
    "    else: raise ValueError()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ckp_gens(model, path_to_ckp, dataset, tokenizer):\n",
    "    if model is not None:\n",
    "        model = PeftModel.from_pretrained(model=model, model_id=path_to_ckp)\n",
    "    else:\n",
    "        model = EncoderDecoderModel.from_pretrained(path_to_ckp, local_files_only=True)\n",
    "    \n",
    "    # adding metrics requires to pass trg_lang\n",
    "    # metrics = SacreBleu.get_mBART_metric(tokenizer=tokenizer, target_language=trg_lang)\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    train_args = Seq2SeqTrainingArguments(\n",
    "        report_to=\"none\",\n",
    "        prediction_loss_only=False,\n",
    "        predict_with_generate=True,\n",
    "        bf16=True,\n",
    "        group_by_length=True,\n",
    "        output_dir=\"./.ckp\",\n",
    "        length_column_name=\"length\",\n",
    "        label_smoothing_factor=0.2,\n",
    "        per_device_eval_batch_size=8\n",
    "    )\n",
    "    gen_config = {\n",
    "        \"max_length\" : 256,\n",
    "        \"early_stopping\" : True,\n",
    "        \"no_repeat_ngram_size\" : 4,\n",
    "        \"length_penalty\" : 1.0,\n",
    "        \"num_beams\" : 5\n",
    "    }\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model,\n",
    "        args=train_args,\n",
    "        data_collator=data_collator,\n",
    "        # compute_metrics=metrics\n",
    "    )\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    predictions = trainer.predict(dataset, **gen_config).predictions\n",
    "    predictions_decode = tokenizer[\"decoder_tokenizer\"].batch_decode(predictions, skip_special_tokens=True)\n",
    "        \n",
    "    return predictions_decode\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_generations(type, ckp_name, ckp_nums, dataset, src_lang):\n",
    "    assert type in [\"mBART\", \"BERT-GPT2-xattn\", \"BERT-GPT2-xattn-LoRA\"], \"invalid type\"\n",
    "    assert os.path.exists(f\"./.ckp/{ckp_name}\"), \"invalid ckp id\"\n",
    "    assert src_lang in [\"en\", \"ja\"], \"invalid language\"\n",
    "    trg_lang = \"ja\" if src_lang == \"en\" else \"en\"\n",
    "    \n",
    "    # get all checkpoints\n",
    "    ckps = []\n",
    "    p_num = re.compile(\".*-(.*)$\")\n",
    "    for fname in pathlib.Path(f\"./.ckp/{ckp_name}\").glob(\"*\"):\n",
    "        ckps.append(int(p_num.match(str(fname)).groups()[0]))\n",
    "    ckps.sort()\n",
    "    assert all(num in ckps for num in ckp_nums), \"ckp_nums is invalid\"\n",
    "    \n",
    "    # generate tokenizer, dataset, model\n",
    "    tokenizers = get_tokenizer(type, src_lang)\n",
    "    model = get_base_model(type, src_lang=src_lang)\n",
    "    processed = tokenize_dataset(dataset, src_lang, type, **tokenizers)\n",
    "    \n",
    "    # generate predictions with given model\n",
    "    gens = {}\n",
    "    for ckp in ckp_nums:\n",
    "        path_to_ckp = f\"./.ckp/{ckp_name}/checkpoint-{ckp}\"\n",
    "        gen = compute_ckp_gens(model, path_to_ckp, processed, tokenizers[\"decoder_tokenizer\"])\n",
    "        gens[f\"pred@{ckp}\"] = gen\n",
    "    \n",
    "    # create new dataset with source, target and predictions\n",
    "    for key in dataset:\n",
    "        gens[key] = dataset[key]\n",
    "    data = Dataset.from_dict(gens)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation (en, ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    \"source\": [\n",
    "        \"The number of users of the Yahoo! and Microsoft services combined will rival the number of AOL's customers.\",\n",
    "        \"The game publisher Konami stated today in a Japanese newspaper that they will not be releasing the game Six Days in Fallujah.\",\n",
    "        \"Present-day parts of Belgium were part of Luxembourg in the past but became Belgian after the 1830s Belgian Revolution.\",\n",
    "        \n",
    "        \"many animals have been destroyed by men .\",\n",
    "        \"it saved me .\",\n",
    "        \"i don 't blame you .\",\n",
    "        \n",
    "        \"There were two to three appointed to the post.\",\n",
    "        \"In the last 3 months, over 80 arrestees were released from the Central Booking facility without being formally charged.\",\n",
    "        \"He joined them in 1945 and stayed until 1958.\",\n",
    "        \n",
    "        \"An additional 300 brings the total to 1,300 carriages to be acquired to relieve overcrowding.\",\n",
    "        \"There are ten dogs and five cats in this house.\",\n",
    "        \"There are three dolphins.\",\n",
    "        \n",
    "        \"Today, I picked up a 10,000-yen bill.\",\n",
    "        \"There are three minors aged 18.\"\n",
    "    ],\n",
    "    \"target\": [\n",
    "        \"ヤフーとマイクロソフトのサービスを合わせたユーザー数は、AOLの顧客数に匹敵するだろう。\",\n",
    "        \"ゲームメーカーのコナミは本日、日本の新聞で、「Six Days in Fallujah」というゲームをリリースしないことを明言しました。\",\n",
    "        \"現在のベルギー領の一部は過去にルクセンブルク領でしたが、1830年代のベルギー革命後にベルギー領になりました。\",\n",
    "        \n",
    "        \"多くの動物が人間によって滅ぼされた。\",\n",
    "        \"あなたの存在に助けられたよ。\",\n",
    "        \"あなたがそうするのは当然だ。\",\n",
    "        \n",
    "        \"定員は2～3人。\",\n",
    "        \"過去3カ月間に、80人以上の逮捕者が正式に起訴されることなくセントラルブッキング施設から釈放されました。\",\n",
    "        \"1945年に彼らと合流し、1958年まで滞在した。\",\n",
    "        \n",
    "        \"混雑を緩和するために、300両を追加して計1,300両が確保される予定です。\",\n",
    "        \"この家には犬が10匹、猫が5匹います。\",\n",
    "        \"イルカが3頭います。\",\n",
    "        \n",
    "        \"きょう、1万円札を拾いました。\",\n",
    "        \"18歳の未成年が3人います。\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "ja_additions = {\n",
    "    \"source\": [\n",
    "        \"このエレベーターは一度に１０人運べる。\", \n",
    "        \"このエレベーターは１０人運ぶことができる。\",\n",
    "        \"やることはいくらでもある。\",\n",
    "        \"やることは山ほどある。\",\n",
    "        \"やることがたくさんある。\",\n",
    "        \"こちらは娘です。\",\n",
    "        \"この子は私の娘です。\",\n",
    "        \"これは娘です。\",\n",
    "        \"当地に来てからどのくらいになりますか。\",\n",
    "        \"ここに来てどのくらい？\",\n",
    "        \"ここに来て、どれくらいになるの？\",\n",
    "        \"あなたはどれぐらいの時間ここにいるのですか。\",\n",
    "        \"すみません、田中先生はいますか。\",\n",
    "        \"すみません、田中先生はいらっしゃいますか\",\n",
    "        \"私はさくらです。\",\n",
    "        \"私はさくらと申します。\",\n",
    "        \"ぺらぺら\", \n",
    "        \"どきどき\", \n",
    "        \"ぺこぺこ\"\n",
    "    ],\n",
    "    \"target\": [None] * 19\n",
    "}\n",
    "\n",
    "en_additions = {\n",
    "    \"source\": [\n",
    "        \"This elevator is capable of carrying 10 persons at a time.\",\n",
    "        \"I have a ton of stuff to do.\",\n",
    "        \"This is my daughter.\",\n",
    "        \"How long have you been here?\",\n",
    "        \"Is Mr. Tanaka here?\",\n",
    "        \"I am Sakura.\"\n",
    "    ],\n",
    "    \"target\": [None] * 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"source\": dataset[\"source\"].copy(),\n",
    "    \"target\": dataset[\"target\"].copy(),\n",
    "}\n",
    "data[\"source\"].extend(en_additions[\"source\"])\n",
    "data[\"target\"].extend(en_additions[\"target\"])\n",
    "assert len(data[\"source\"]) == len(data[\"target\"])\n",
    "\n",
    "gen_base      = compute_generations(\"mBART\", \"en-ja-mixed-250-250k+bt-250k\", [2500, 25000]       , dataset, \"en\")\n",
    "\n",
    "gen_extended  = compute_generations(\"mBART\", \"en-ja-mixed-500k\"            , [2500, 45000, 50000], dataset, \"en\")\n",
    "\n",
    "gen_baseandbt = compute_generations(\"mBART\", \"en-ja-ckp-25000-bt-500k\"     , [2500, 35000, 50000], dataset, \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation (ja, en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"source\": dataset[\"source\"].copy(),\n",
    "    \"target\": dataset[\"target\"].copy(),\n",
    "}\n",
    "data[\"source\"].extend(ja_additions[\"source\"])\n",
    "data[\"target\"].extend(ja_additions[\"target\"])\n",
    "assert len(data[\"source\"]) == len(data[\"target\"])\n",
    "\n",
    "gen_base      = compute_generations(\"mBART\", \"ja-en-mixed-250-250k+bt-250k\", [2500, 25000]       , dataset, \"ja\")\n",
    "\n",
    "gen_extended  = compute_generations(\"mBART\", \"ja-en-mixed-500k\"            , [2500, 45000, 50000], dataset, \"ja\")\n",
    "\n",
    "gen_baseandbt = compute_generations(\"mBART\", \"ja-en-ckp-25000-bt-500k\"     , [2500, 35000, 50000], dataset, \"ja\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
