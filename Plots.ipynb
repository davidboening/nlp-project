{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_EVAL_FOLDER = \"./.eval/\"\n",
    "\n",
    "# manually added data\n",
    "extra_data = {'en-ja-BERT-GPT2-LoRA': dict([\n",
    "    (2500, 0.6847),(5000, 1.046),(7500, 1.838),(10000, 2.187),(12500, 2.648),\n",
    "    (15000, 2.998), (17500, 2.982), (20000, 3.290), (22500, 3.681), (25000, 3.597)\n",
    "])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(name):\n",
    "    p = re.compile(r\".*\\\\(.*)\\\\.*\")\n",
    "    data = {}\n",
    "    for fname in Path(BASE_EVAL_FOLDER).glob(f\"*/{name}\"):\n",
    "        if \"news\" in str(fname): \n",
    "            continue # skip news invalid format\n",
    "        with fname.open(\"rb\") as fp:\n",
    "            ckp_data = json.load(fp)\n",
    "            ckp_data = dict((int(k), ckp_data[k][\"test_score\"]) for k in ckp_data.keys())\n",
    "            data[p.match(str(fname)).groups()[0]] = ckp_data\n",
    "    return data\n",
    "\n",
    "def convert_to_df_0(data):\n",
    "    a = []\n",
    "    for k1, d in data.items():\n",
    "        for k2, v in d.items():\n",
    "            if k1.endswith(\"-BERT-GPT2-xattn\"):\n",
    "                a.append({\"model\" : f\"BERT-GPT2 (xattn) [{k1[:5]}]\", \"steps\": k2, \"score\": v})\n",
    "            elif k1.endswith(\"-BERT-GPT2-LoRA\"):\n",
    "                a.append({\"model\" : f\"BERT-GPT2 (xattn+LoRA) [{k1[:5]}]\", \"steps\": k2, \"score\": v})\n",
    "            elif k1.endswith(\"-BERT-GPT2-xattn-LoRA\"):\n",
    "                a.append({\"model\" : f\"BERT-GPT2 (xattn->LoRA) [{k1[:5]}]\", \"steps\": k2, \"score\": v})\n",
    "            elif k1.endswith(\"+bt-250k\") and k1.startswith(\"en-ja\"):\n",
    "                a.append({\"model\" : f\"mBART [{k1[:5]}]\", \"steps\": k2, \"score\": v})\n",
    "    df = pd.DataFrame(a)\n",
    "    return df\n",
    "\n",
    "def convert_to_df_1(data):\n",
    "    a = []\n",
    "    for k1, d in data.items():\n",
    "        for k2, v in d.items():\n",
    "            if k1.endswith(\"-bt-500k\"):\n",
    "                a.append({\"model\" : f\"base+BT [{k1[:5]}]\", \"steps\": k2, \"score\": v})\n",
    "            elif k1.endswith(\"+bt-250k\"):\n",
    "                a.append({\"model\" : f\"base [{k1[:5]}]\", \"steps\": k2, \"score\": v})\n",
    "            elif k1.endswith(\"-mixed-500k\"):\n",
    "                a.append({\"model\" : f\"extended [{k1[:5]}]\", \"steps\": k2, \"score\": v})\n",
    "    df = pd.DataFrame(a)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT-GPT2 vs mBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_scores(\"flores_dev.json\")\n",
    "# data.update(extra_data)\n",
    "px.line(\n",
    "    convert_to_df_0(data), \n",
    "    x=\"steps\", y=\"score\", color=\"model\", range_x=(2500, 25000), # range_y=(9, 15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_df_0(load_scores(\"wmt_vat.json\")).groupby(\"model\").last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mBART "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    convert_to_df_1(load_scores(\"flores_dev.json\")), \n",
    "    x=\"steps\", y=\"score\", color=\"model\", range_x=(5000, 50000), range_y=(9, 15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_to_df_1(load_scores(\"flores_dev.json\"))\n",
    "for model_type in data.model.unique():\n",
    "    data1 = data[data.model == model_type]\n",
    "    print(dict(data1.loc[data1.score.idxmax()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_df_1(load_scores(\"wmt_vat.json\")).groupby(\"model\").last()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
