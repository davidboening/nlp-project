{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, \\\n",
    "    GenerationConfig, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "# from transformers.utils import logging\n",
    "# logging.set_verbosity_info()\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "\n",
    "SOURCE_LANG = \"en\"\n",
    "\n",
    "if SOURCE_LANG == \"en\":\n",
    "    TARGET_LANG = \"ja\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ja_XX\")\n",
    "else: \n",
    "    TARGET_LANG = \"en\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"ja_XX\", tgt_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import EnJaDatasetMaker\n",
    "dataset = EnJaDatasetMaker.load_dataset(f\"{SOURCE_LANG}-{TARGET_LANG}-final\")\n",
    "train_data = dataset[\"train\"]\n",
    "valid_data = dataset[\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "if TARGET_LANG == \"ja\":\n",
    "    def compute_metrics(preds):\n",
    "        preds_ids, labels_ids = preds\n",
    "\n",
    "        labels_ids[labels_ids == -100] = tokenizer.eos_token_id\n",
    "        references = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "        references = [[reference] for reference in references]\n",
    "\n",
    "        predictions = tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
    "\n",
    "        bleu_output = metric.compute(\n",
    "            references=references, \n",
    "            predictions=predictions, \n",
    "            tokenize=\"ja-mecab\"\n",
    "        )\n",
    "        return bleu_output\n",
    "else:\n",
    "    def compute_metrics(preds):\n",
    "        preds_ids, labels_ids = preds\n",
    "\n",
    "        labels_ids[labels_ids == -100] = tokenizer.eos_token_id\n",
    "        references = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "        references = [[reference] for reference in references]\n",
    "\n",
    "        predictions = tokenizer.batch_decode(preds_ids, skip_special_tokens=True)\n",
    "        \n",
    "        bleu_output = metric.compute(\n",
    "            references=references, \n",
    "            predictions=predictions\n",
    "        )\n",
    "        return bleu_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = Seq2SeqTrainingArguments(\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"{SOURCE_LANG}-{TARGET_LANG}-mBART-base\",\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1, # * 4, 2, 1\n",
    "\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000, # * 20_000, 10_000, 5_000\n",
    "    prediction_loss_only=False,\n",
    "    predict_with_generate=True,\n",
    "    generation_config=gen_config,\n",
    "\n",
    "    output_dir=\"./.ckp/\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5_000, # * 20_000, 10_000, 5_000\n",
    "    save_total_limit=20,\n",
    "    load_best_model_at_end=True, # defaults to metric: \"loss\"\n",
    "    metric_for_best_model=\"eval_score\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_steps=875, # 3500, 1750, 875\n",
    "    learning_rate=3e-5, # 3e-5, 5e-5\n",
    "    bf16=True, # bf16, qint 8 ???\n",
    "    \n",
    "    group_by_length=True,\n",
    "    length_column_name=\"length\",\n",
    "\n",
    "    # torch_compile=True,\n",
    "    label_smoothing_factor=0.2, # 0.1, 0.2\n",
    "    \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4, # * 1, 2, 4\n",
    "    # eval_accumulation_steps=4, # ???\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import Flores\n",
    "\n",
    "flores_dev_data = Flores.load(\"dev\").rename_columns({f\"{SOURCE_LANG}_sentence\": \"source\", f\"{TARGET_LANG}_sentence\": \"target\"})\n",
    "\n",
    "flores_dev_data = flores_dev_data.map(\n",
    "    EnJaDatasetMaker._get_map_compute_mBART_tokenization(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flores_test_data = Flores.load(\"dev\").rename_columns({f\"{SOURCE_LANG}_sentence\": \"source\", f\"{TARGET_LANG}_sentence\": \"target\"})\n",
    "\n",
    "flores_test_data = flores_test_data.map(\n",
    "    EnJaDatasetMaker._get_map_compute_mBART_tokenization(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import WMTvat\n",
    "\n",
    "wmt_data = WMTvat.load(f\"{SOURCE_LANG}-{TARGET_LANG}\").rename_columns({f\"{SOURCE_LANG}_sentence\": \"source\", f\"{TARGET_LANG}_sentence\": \"target\"})\n",
    "\n",
    "wmt_data = wmt_data.map(\n",
    "    EnJaDatasetMaker._get_map_compute_mBART_tokenization(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint evaluation\n",
    "Evaluate sacreBLEU score on all checkpoints saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "flores_dev_results = {}\n",
    "flores_test_results = {}\n",
    "wmt_results = {}\n",
    "\n",
    "for i in range(5000, 55000, 5000):\n",
    "\n",
    "    lora_model = PeftModel.from_pretrained(model=model, model_id=f\"./.ckp_{SOURCE_LANG}_{TARGET_LANG}/checkpoint-{i}/\")\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=lora_model)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        lora_model,\n",
    "        args=train_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=valid_data,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    lora_model.cuda()\n",
    "    lora_model.eval()\n",
    "\n",
    "    flores_dev_results[f\"checkpoint_{i}\"] = trainer.predict(flores_dev_data).metrics\n",
    "\n",
    "    flores_test_results[f\"checkpoint_{i}\"] = trainer.predict(flores_test_data).metrics\n",
    "\n",
    "    wmt_results[f\"checkpoint_{i}\"] = trainer.predict(wmt_data).metrics\n",
    "\n",
    "    with open(f\"{SOURCE_LANG}_{TARGET_LANG}_flores_dev_results.json\", \"w\") as f:\n",
    "        f.write(json.dumps(flores_dev_results))\n",
    "\n",
    "    with open(f\"{SOURCE_LANG}_{TARGET_LANG}_flores_test_results.json\", \"w\") as f:\n",
    "        f.write(json.dumps(flores_test_results))\n",
    "\n",
    "    with open(f\"{SOURCE_LANG}_{TARGET_LANG}_wmt_results.json\", \"w\") as f:\n",
    "        f.write(json.dumps(wmt_results))\n",
    "\n",
    "    print(\"Checkpoint \", i, \" DONE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample examination\n",
    "Translate a given dataset with the chosen checkpoint to examine get sacreBLEU score and examine translation quality on samples of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 50000\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(model=model, model_id=f\"./.ckp_{SOURCE_LANG}_{TARGET_LANG}/checkpoint-{checkpoint}/\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=lora_model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    lora_model,\n",
    "    args=train_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_decoder_configuration(gc: GenerationConfig):\n",
    "    gc.no_repeat_ngram_size = 4\n",
    "    gc.length_penalty = 2.0\n",
    "    gc.num_beams = 1\n",
    "    # gc.max_new_tokens = 128\n",
    "    gc.max_length = 256\n",
    "    gc.min_length = 0\n",
    "    gc.early_stopping = True\n",
    "    # pad token is set to eos since in GPT2 pad does not exist\n",
    "    gc.pad_token_id = tokenizer.eos_token_id\n",
    "    gc.bos_token_id = tokenizer.bos_token_id\n",
    "    gc.eos_token_id = tokenizer.eos_token_id\n",
    "    gc.do_sample = False\n",
    "    gc.penalty_alpha = 0.2\n",
    "    gc.top_k = 10\n",
    "    return gc\n",
    "\n",
    "gen_config = GenerationConfig()\n",
    "gen_config = set_decoder_configuration(gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.cuda()\n",
    "lora_model.eval()\n",
    "\n",
    "data = valid_data\n",
    "\n",
    "predictions = trainer.predict(valid_data, gen_config)\n",
    "\n",
    "print(\"Metrics: \", predictions.metrics)\n",
    "\n",
    "predictions_decode = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "\n",
    "def print_pairs(dataset, generation, sample=5):\n",
    "    assert len(dataset) == len(generation), \"Invalid combination!\"\n",
    "\n",
    "    sample_ids = random.sample(range(len(dataset)), sample)\n",
    "    for i, sid in enumerate(sample_ids):\n",
    "        print(f\"Sentence #{i} [id={sid}]\")\n",
    "        print(\n",
    "            \"\\n\\t\\t\\t\".join(wrap(f\"\\tOriginal:  {dataset['source'][sid]}\", width=100)),\n",
    "            \"\\n\\t\\t\\t\".join(wrap(f\"\\tTarget:    {dataset['target'][sid]}\", width=100)),\n",
    "            \"\\n\\t\\t\\t\".join(wrap(f\"\\tGenerated: {generation[sid]}\", width=100)), sep=\"\\n\"\n",
    "        )\n",
    "        print(\"\\n\")\n",
    "    return\n",
    "\n",
    "print_pairs(data, predictions_decode, sample=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
