{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, \\\n",
    "    Seq2SeqTrainer, DataCollatorForSeq2Seq, GenerationConfig, Seq2SeqTrainingArguments\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from utils.dataset import EnJaDatasetMaker, EnJaBackTranslation\n",
    "from utils.metric import SacreBleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "\n",
    "SOURCE_LANG = \"en\"\n",
    "\n",
    "if SOURCE_LANG == \"en\":\n",
    "    TARGET_LANG = \"ja\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ja_XX\")\n",
    "else: \n",
    "    TARGET_LANG = \"en\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"ja_XX\", tgt_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = GenerationConfig(\n",
    "    no_repeat_ngram_size = 4,\n",
    "    length_penalty = 1.0,\n",
    "    num_beams = 3,\n",
    "    max_length = 256,\n",
    "    min_length = 0,\n",
    "    early_stopping = True,\n",
    "    # pad token is set to eos since in GPT2 pad does not exist\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    bos_token_id = tokenizer.bos_token_id,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "train_args = Seq2SeqTrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    # run_name=f\"{SOURCE_LANG}-{TARGET_LANG}-mBART-base\",\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1, # * 4, 2, 1\n",
    "\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000, # * 20_000, 10_000, 5_000\n",
    "    prediction_loss_only=False,\n",
    "    predict_with_generate=True,\n",
    "    generation_config=gen_config,\n",
    "\n",
    "    output_dir=\"./.ckp/\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=5_000, # * 20_000, 10_000, 5_000\n",
    "    save_total_limit=20,\n",
    "    load_best_model_at_end=True, # defaults to metric: \"loss\"\n",
    "    metric_for_best_model=\"eval_score\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_steps=875, # 3500, 1750, 875\n",
    "    learning_rate=3e-5, # 3e-5, 5e-5\n",
    "    bf16=True, # bf16, qint 8 ???\n",
    "    \n",
    "    group_by_length=True,\n",
    "    length_column_name=\"length\",\n",
    "\n",
    "    # torch_compile=True,\n",
    "    label_smoothing_factor=0.2, # 0.1, 0.2\n",
    "    \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4, # * 1, 2, 4\n",
    "    # eval_accumulation_steps=4, # ???\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = PeftModel.from_pretrained(model=model, model_id=r\"./.ckp_en_ja_hq_news/checkpoint-21000/\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=lora_model)\n",
    "metric = SacreBleu.get_mBART_metric(tokenizer=tokenizer, target_language=TARGET_LANG)\n",
    "\n",
    "# wrap for easier prediction/generation\n",
    "trainer = Seq2SeqTrainer(\n",
    "    lora_model,\n",
    "    args=train_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=None,\n",
    "    eval_dataset=None,\n",
    "    compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data : Dataset = EnJaDatasetMaker.load_dataset(f\"{SOURCE_LANG}-{TARGET_LANG}-hq-news\")[\"train\"].select(range(1000))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ID column for consistent ordering\n",
    "data = data.add_column(\"id\", list(range(len(data))))\n",
    "# sort by length for efficient dynamic padding\n",
    "data = data.sort(column_names=[\"length\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnJaBackTranslation.create_mBART_backtranslation(\n",
    "    trainer, data, SOURCE_LANG, tokenizer, \n",
    "    gen_config=gen_config, chunk_size=100, out_dir=\"./data-bt\", \n",
    "    out_name=f\"{TARGET_LANG}-{SOURCE_LANG}-hq-news-mBART-bt.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
