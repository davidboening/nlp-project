{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, \\\n",
    "    Seq2SeqTrainer, DataCollatorForSeq2Seq, GenerationConfig, Seq2SeqTrainingArguments\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset\n",
    "from utils.dataset import EnJaDatasetMaker, EnJaBackTranslation\n",
    "from utils.metric import SacreBleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "\n",
    "SOURCE_LANG = \"en\"\n",
    "\n",
    "if SOURCE_LANG == \"en\":\n",
    "    TARGET_LANG = \"ja\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ja_XX\")\n",
    "else: \n",
    "    TARGET_LANG = \"en\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"ja_XX\", tgt_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = {\n",
    "    \"max_length\" : 256,\n",
    "    \"early_stopping\" : True,\n",
    "    \n",
    "    \"no_repeat_ngram_size\" : 4,\n",
    "    \"length_penalty\" : 1.0,\n",
    "    \n",
    "    \"num_beams\" : 5,\n",
    "    # \"num_beam_groups\" : 5,\n",
    "    # \"diversity_penalty\" : 0.5,\n",
    "    # \"do_sample\" : True,\n",
    "    # \"penalty_alpha\" : 0.6,\n",
    "    # \"top_k\" : 4,\n",
    "}\n",
    "\n",
    "train_args = Seq2SeqTrainingArguments(\n",
    "    report_to=\"none\",\n",
    "\n",
    "    prediction_loss_only=False,\n",
    "    predict_with_generate=True,\n",
    "\n",
    "    bf16=True,\n",
    "    output_dir=\"./ckp\",\n",
    "    \n",
    "    group_by_length=True,\n",
    "    length_column_name=\"length\",\n",
    "\n",
    "    label_smoothing_factor=0.2,\n",
    "    \n",
    "    per_device_eval_batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"mixed-250k+bt-250k\"\n",
    "CHECKPOINT = 25_000\n",
    "\n",
    "data : Dataset = EnJaDatasetMaker.load_dataset(f\"{SOURCE_LANG}-{TARGET_LANG}-{DATASET_NAME}\")[\"test\"]\n",
    "# add ID column for consistent ordering\n",
    "data = data.add_column(\"id\", list(range(len(data))))\n",
    "# sort by length for efficient dynamic padding\n",
    "data = data.sort(column_names=[\"length\", \"id\"])\n",
    "\n",
    "# load and apply adapter\n",
    "lora_model = PeftModel.from_pretrained(model=model,\n",
    "    model_id=f\"./.ckp/{SOURCE_LANG}-{TARGET_LANG}-{DATASET_NAME}/checkpoint-{CHECKPOINT}\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=lora_model)\n",
    "metric = SacreBleu.get_mBART_metric(tokenizer=tokenizer, target_language=TARGET_LANG)\n",
    "\n",
    "# wrap for easier prediction/generation\n",
    "trainer = Seq2SeqTrainer(\n",
    "    lora_model,\n",
    "    args=train_args,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnJaBackTranslation.create_mBART_backtranslation(\n",
    "    trainer, data, SOURCE_LANG, tokenizer, \n",
    "    gen_config=gen_config, chunk_size=1_000, out_dir=\"./data-bt\", \n",
    "    out_name=f\"{TARGET_LANG}-{SOURCE_LANG}-ckp-{CHECKPOINT}-bt.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Datasets : train + BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"./.cache\"\n",
    "from utils.dataset import EnJaDatasetSample, EnJaDatasetMaker\n",
    "from transformers import MBart50TokenizerFast\n",
    "from datasets import concatenate_datasets, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"mixed-250k+bt-250k\"\n",
    "CHECKPOINT = 25_000\n",
    "SOURCE_LANG = \"ja\"\n",
    "\n",
    "if SOURCE_LANG == \"en\":\n",
    "    TARGET_LANG = \"ja\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"en_XX\", tgt_lang=\"ja_XX\")\n",
    "else: \n",
    "    TARGET_LANG = \"en\"\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"ja_XX\", tgt_lang=\"en_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10518d35142d4d8097f825a62d4810f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/273994 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa43a31724614e989e0515c7df8a81e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/273994 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling: using all data (273970)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbddca2c7bb46b78e68466a81089ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/273423 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5251e45323a9455fba45ff3d55bdaffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/547 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_data = EnJaDatasetMaker.load_dataset(f\"{SOURCE_LANG}-{TARGET_LANG}-{DATASET_NAME}\")\n",
    "bt_data = EnJaDatasetMaker.prepare_dataset(\n",
    "    f\"{SOURCE_LANG}-{TARGET_LANG}-bt-only\",\n",
    "    [\n",
    "        EnJaDatasetSample(\n",
    "            dataset=f\"./data-bt/{SOURCE_LANG}-{TARGET_LANG}-ckp-{CHECKPOINT}-bt.csv\", \n",
    "            nsample=300_000, ntokens=(0, 128)\n",
    "        ),\n",
    "    ],\n",
    "    source_language = SOURCE_LANG,\n",
    "    model_type= \"mBART\",\n",
    "    tokenizer = tokenizer,\n",
    "    num_proc  = 8,\n",
    "    seed      = 123,\n",
    "    splits    = (1, 0.002) # rescaled to 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tr_data = concatenate_datasets([tr_data[\"train\"], bt_data[\"train\"]])\n",
    "full_va_data = concatenate_datasets([tr_data[\"valid\"], bt_data[\"test\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e42ec973000457bb0ff210150110c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/546409 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c908514367446e0bd6f229f10d85e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1088 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_data = DatasetDict({\"train\" : full_tr_data, \"valid\" : full_va_data})\n",
    "full_data = full_data.shuffle(42)\n",
    "full_data.save_to_disk(f\"./data-fin/{SOURCE_LANG}-{TARGET_LANG}-ckp-{CHECKPOINT}-bt-500k\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
